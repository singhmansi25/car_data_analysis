<h1 id="data-analysis-automobile-dataset">DATA ANALYSIS : Automobile Dataset</h1>
<hr />
<h2 id="problem"><code>Problem</code></h2>
<div class="container alert alert-danger alertdanger">
<p>Let’s say we have a friend named Tom. And Tom wants to sell his car. But the problem is he doesn’t know how much he should sell his car for. Tom wants to sell his car for as much as he can. But he also wants to set the price reasonably, so someone would want to purchase it. So the price he sets should represent the value of the car.How can we help Tom determine the best price for his car? Let’s think like data scientists and clearly define some of his problems. For example, is there data on the prices of other cars and their characteristics? What features of cars affect their prices? Color? Brand? Does horsepower also effect the selling price, or perhaps something else? As a data analyst or data scientist, these are some of the questions we can start thinking about. To answer these questions, we’re going to need some data.</p>
</div>
<p><strong>``The final model has efficiency of 84%`` and below one is it’s performance graph</strong></p>
<pre class="ipython3"><code># DistributionPlot(y_test, yhat, &quot;Actual Values (Test)&quot;, &quot;Predicted Values (Test)&quot;, Title)</code></pre>
<h2 id="table-of-content"><code>TABLE OF CONTENT</code></h2>
<hr />
<div class="container alert alert-danger alertdanger">
<ol>
<li>
<p>Data Acquisition</p>
</li>
<li>
<p>Identify and handle missing values</p>
</li>
<li>
<p>Data Standardization</p>
</li>
<li>
<p>Data Normalization</p>
</li>
<li>
<p>Binning</p>
</li>
<li>
<p>Analyzing Individual Feature Patterns using Visualization</p>
</li>
<li>
<p>Model Development</p>
</li>
<ul>
<li>
<p>Linear Regression and Multiple Linear Regression</p>
</li>
<li>
<p>Model Evaluation using Visualization</p>
</li>
<li>
<p>Polynomial Regression and Pipeline</p>
</li>
<li>
<p>Measures for Insample Evaluation</p>
</li>
<li>
<p>Prediction and Decision Making</p>
</li>
</ul>
<li>
<p>Model Evaluation and Refinement</p>
</li>
<li>
<p>Conclusion</p>
</li>
<li>
<p>Reference</p>
</li>
</ol>
</div>
<h2 id="data-acquisition"><code>1.Data Acquisition</code></h2>
<div class="container alert alert-danger alertdanger">
<p>There are various formats for a dataset, .csv, .json, .xlsx etc. The dataset can be stored in different places, on your local machine or sometimes online.In our case, the Automobile Dataset is an online source, and it is in CSV (comma separated value) format. Let’s use this dataset as an example to practice data reading.</p>
<ul>
<li>
<p>data source: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data">https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data</a></p>
</li>
<li>
<p>date type : .csv</p>
</li>
</ul>
<p>The Pandas Library is a useful tool that enables us to read various datasets into a data frame;so that all we need to do is import Pandas. If you crossed by error, install it first.</p>
<p>We use pandas.read_csv() function to read the csv file. In the bracket, we put the file path along with a quotation mark, so that pandas will read the file into a data frame from that address. The file path can be either an URL or your local file address. Because the data does not include headers, we can add an argument headers = None inside the read_csv() method, so that pandas will not automatically set the first row as a header. You can also assign the dataset to any variable you create.</p>
</div>
<pre class="ipython3"><code>! pip install ipywidgets --upgrade --quiet</code></pre>
<div class="parsed-literal">
<p>WARNING: You are using pip version 22.0.4; however, version 22.1.1 is available. You should consider upgrading via the 'D:Pythonpython.exe -m pip install --upgrade pip' command.</p>
</div>
<div class="container alert alert-block alert-info">
<p>import helper libraries</p>
</div>
<pre class="ipython3"><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import pyplot
%matplotlib inline
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
from ipywidgets import interact, interactive, fixed, interact_manual</code></pre>
<div class="container alert alert-block alert-info">
<p>Read the online file by the URL provides above, and assign it to variable “df”</p>
</div>
<pre class="ipython3"><code>url = &quot;https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv&quot;
df = pd.read_csv(url, header=None)</code></pre>
<div class="container alert alert-block alert-info">
<p>After reading the dataset, we can use the dataframe.head(n) method to check the top n rows of the dataframe; where n is an integer.</p>
</div>
<pre class="ipython3"><code># show the first 5 rows using dataframe.head() method
print(&quot;The first 5 rows of the dataframe&quot;) 
df.head(5)</code></pre>
<div class="parsed-literal">
<p>The first 5 rows of the dataframe</p>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>13495</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>...</td>
      <td>152</td>
      <td>mpfi</td>
      <td>2.68</td>
      <td>3.47</td>
      <td>9.0</td>
      <td>154</td>
      <td>5000</td>
      <td>19</td>
      <td>26</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>109</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>10.0</td>
      <td>102</td>
      <td>5500</td>
      <td>24</td>
      <td>30</td>
      <td>13950</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.0</td>
      <td>115</td>
      <td>5500</td>
      <td>18</td>
      <td>22</td>
      <td>17450</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 26 columns</p>
</div>
<div class="container alert alert-block alert-info">
<p>Contrary to dataframe.head(n), dataframe.tail(n) will show you the bottom n rows of the dataframe.</p>
</div>
<pre class="ipython3"><code>print(&quot;The last 5 rows of the dataframe&quot;) 
df.tail(10)</code></pre>
<div class="parsed-literal">
<p>The last 5 rows of the dataframe</p>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>195</th>
      <td>-1</td>
      <td>74</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>wagon</td>
      <td>rwd</td>
      <td>front</td>
      <td>104.3</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>23</td>
      <td>28</td>
      <td>13415</td>
    </tr>
    <tr>
      <th>196</th>
      <td>-2</td>
      <td>103</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>104.3</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>24</td>
      <td>28</td>
      <td>15985</td>
    </tr>
    <tr>
      <th>197</th>
      <td>-1</td>
      <td>74</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>wagon</td>
      <td>rwd</td>
      <td>front</td>
      <td>104.3</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>24</td>
      <td>28</td>
      <td>16515</td>
    </tr>
    <tr>
      <th>198</th>
      <td>-2</td>
      <td>103</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>104.3</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.62</td>
      <td>3.15</td>
      <td>7.5</td>
      <td>162</td>
      <td>5100</td>
      <td>17</td>
      <td>22</td>
      <td>18420</td>
    </tr>
    <tr>
      <th>199</th>
      <td>-1</td>
      <td>74</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>wagon</td>
      <td>rwd</td>
      <td>front</td>
      <td>104.3</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.62</td>
      <td>3.15</td>
      <td>7.5</td>
      <td>162</td>
      <td>5100</td>
      <td>17</td>
      <td>22</td>
      <td>18950</td>
    </tr>
    <tr>
      <th>200</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>23</td>
      <td>28</td>
      <td>16845</td>
    </tr>
    <tr>
      <th>201</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>8.7</td>
      <td>160</td>
      <td>5300</td>
      <td>19</td>
      <td>25</td>
      <td>19045</td>
    </tr>
    <tr>
      <th>202</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>173</td>
      <td>mpfi</td>
      <td>3.58</td>
      <td>2.87</td>
      <td>8.8</td>
      <td>134</td>
      <td>5500</td>
      <td>18</td>
      <td>23</td>
      <td>21485</td>
    </tr>
    <tr>
      <th>203</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>diesel</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>145</td>
      <td>idi</td>
      <td>3.01</td>
      <td>3.40</td>
      <td>23.0</td>
      <td>106</td>
      <td>4800</td>
      <td>26</td>
      <td>27</td>
      <td>22470</td>
    </tr>
    <tr>
      <th>204</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>19</td>
      <td>25</td>
      <td>22625</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 26 columns</p>
</div>
<h2 id="identify-and-handle-missing-values"><code>2.IDENTIFY AND HANDLE MISSING VALUES</code></h2>
<h3 id="add-headers"><code>ADD HEADERS</code></h3>
<div class="container alert alert-block alert-info">
<p>
<p>Take a look at our dataset; pandas automatically set the header by an integer from 0.</p>
</p>
<p>
<p>To better describe our data we can introduce a header, this information is available at: <a href="https://archive.ics.uci.edu/ml/datasets/Automobile">https://archive.ics.uci.edu/ml/datasets/Automobile</a></p>
</p>
<p>
<p>Thus, we have to add headers manually.</p>
</p>
<p>
<p>Firstly, we create a list “headers” that include all column names in order. Then, we use dataframe.columns = headers to replace the headers by the list we created.</p>
</p>
</div>
<pre class="ipython3"><code># create headers list
headers = [&quot;symboling&quot;,&quot;normalized-losses&quot;,&quot;make&quot;,&quot;fuel-type&quot;,&quot;aspiration&quot;, &quot;num-of-doors&quot;,&quot;body-style&quot;,
         &quot;drive-wheels&quot;,&quot;engine-location&quot;,&quot;wheel-base&quot;, &quot;length&quot;,&quot;width&quot;,&quot;height&quot;,&quot;curb-weight&quot;,&quot;engine-type&quot;,
         &quot;num-of-cylinders&quot;, &quot;engine-size&quot;,&quot;fuel-system&quot;,&quot;bore&quot;,&quot;stroke&quot;,&quot;compression-ratio&quot;,&quot;horsepower&quot;,
         &quot;peak-rpm&quot;,&quot;city-mpg&quot;,&quot;highway-mpg&quot;,&quot;price&quot;]

df.columns = headers
df.head(10)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>fuel-type</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>...</th>
      <th>engine-size</th>
      <th>fuel-system</th>
      <th>bore</th>
      <th>stroke</th>
      <th>compression-ratio</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>13495</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>?</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>...</td>
      <td>152</td>
      <td>mpfi</td>
      <td>2.68</td>
      <td>3.47</td>
      <td>9.0</td>
      <td>154</td>
      <td>5000</td>
      <td>19</td>
      <td>26</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>109</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>10.0</td>
      <td>102</td>
      <td>5500</td>
      <td>24</td>
      <td>30</td>
      <td>13950</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.0</td>
      <td>115</td>
      <td>5500</td>
      <td>18</td>
      <td>22</td>
      <td>17450</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>?</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.5</td>
      <td>110</td>
      <td>5500</td>
      <td>19</td>
      <td>25</td>
      <td>15250</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>158</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>105.8</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.5</td>
      <td>110</td>
      <td>5500</td>
      <td>19</td>
      <td>25</td>
      <td>17710</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>?</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>wagon</td>
      <td>fwd</td>
      <td>front</td>
      <td>105.8</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.5</td>
      <td>110</td>
      <td>5500</td>
      <td>19</td>
      <td>25</td>
      <td>18920</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>158</td>
      <td>audi</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>105.8</td>
      <td>...</td>
      <td>131</td>
      <td>mpfi</td>
      <td>3.13</td>
      <td>3.40</td>
      <td>8.3</td>
      <td>140</td>
      <td>5500</td>
      <td>17</td>
      <td>20</td>
      <td>23875</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>?</td>
      <td>audi</td>
      <td>gas</td>
      <td>turbo</td>
      <td>two</td>
      <td>hatchback</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.5</td>
      <td>...</td>
      <td>131</td>
      <td>mpfi</td>
      <td>3.13</td>
      <td>3.40</td>
      <td>7.0</td>
      <td>160</td>
      <td>5500</td>
      <td>16</td>
      <td>22</td>
      <td>?</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 26 columns</p>
</div>
<p><strong>View column names</strong></p>
<pre class="ipython3"><code>df.columns</code></pre>
<div class="parsed-literal">
<dl>
<dt>Index(['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',</dt>
<dd><p>'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price'], dtype='object')</p>
</dd>
</dl>
</div>
<p>As we can see, several question marks appeared in the dataframe; those are missing values which may hinder our further analysis.</p>
<div class="container">
<p>So, how do we identify all those missing values and deal with them?</p>
</div>
<p>How to work with missing data?</p>
<p>Steps for working with missing data:</p>
<ol>
<li>
<p>identify missing data</p>
</li>
<li>
<p>deal with missing data</p>
</li>
<li>
<p>correct data format</p>
</li>
</ol>
<p>we need to replace the “?” symbol with NaN so the dropna() can remove the missing values</p>
<pre class="ipython3"><code>df=df.replace(&#39;?&#39;,np.NaN)
df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>fuel-type</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>...</th>
      <th>engine-size</th>
      <th>fuel-system</th>
      <th>bore</th>
      <th>stroke</th>
      <th>compression-ratio</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>NaN</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>13495</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>NaN</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>NaN</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>...</td>
      <td>152</td>
      <td>mpfi</td>
      <td>2.68</td>
      <td>3.47</td>
      <td>9.0</td>
      <td>154</td>
      <td>5000</td>
      <td>19</td>
      <td>26</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>109</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>10.0</td>
      <td>102</td>
      <td>5500</td>
      <td>24</td>
      <td>30</td>
      <td>13950</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.0</td>
      <td>115</td>
      <td>5500</td>
      <td>18</td>
      <td>22</td>
      <td>17450</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>200</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>23</td>
      <td>28</td>
      <td>16845</td>
    </tr>
    <tr>
      <th>201</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>8.7</td>
      <td>160</td>
      <td>5300</td>
      <td>19</td>
      <td>25</td>
      <td>19045</td>
    </tr>
    <tr>
      <th>202</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>173</td>
      <td>mpfi</td>
      <td>3.58</td>
      <td>2.87</td>
      <td>8.8</td>
      <td>134</td>
      <td>5500</td>
      <td>18</td>
      <td>23</td>
      <td>21485</td>
    </tr>
    <tr>
      <th>203</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>diesel</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>145</td>
      <td>idi</td>
      <td>3.01</td>
      <td>3.40</td>
      <td>23.0</td>
      <td>106</td>
      <td>4800</td>
      <td>26</td>
      <td>27</td>
      <td>22470</td>
    </tr>
    <tr>
      <th>204</th>
      <td>-1</td>
      <td>95</td>
      <td>volvo</td>
      <td>gas</td>
      <td>turbo</td>
      <td>four</td>
      <td>sedan</td>
      <td>rwd</td>
      <td>front</td>
      <td>109.1</td>
      <td>...</td>
      <td>141</td>
      <td>mpfi</td>
      <td>3.78</td>
      <td>3.15</td>
      <td>9.5</td>
      <td>114</td>
      <td>5400</td>
      <td>19</td>
      <td>25</td>
      <td>22625</td>
    </tr>
  </tbody>
</table>
<p>205 rows × 26 columns</p>
</div>
<div class="container alert alert-block alert-info">
<p>Identify_missing_values</p>
<h4>
<p>Evaluating for Missing Data</p>
</h4>
<p>The missing values are converted to default. We use the following functions to identify these missing values. There are two methods to detect missing data:</p>
<ol>
<li>
<p>.isnull()</p>
</li>
<li>
<p>.notnull()</p>
</li>
</ol>
<p>The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.</p>
</div>
<pre class="ipython3"><code>missing_data = df.isnull()
missing_data.head(5)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>fuel-type</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>...</th>
      <th>engine-size</th>
      <th>fuel-system</th>
      <th>bore</th>
      <th>stroke</th>
      <th>compression-ratio</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 26 columns</p>
</div>
<div class="container alert alert-block alert-info">
<p>True stands for missing value, while False stands for not missing value.</p>
<h4>
<p>Count missing values in each column</p>
</h4>
<p>
<p>Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, “True” represents a missing value, “False” means the value is present in the dataset. In the body of the for loop the method “.value_counts()” counts the number of “True” values.</p>
</p>
</div>
<pre class="ipython3"><code>for column in missing_data.columns:
    print(column)
    print (missing_data[column].value_counts())
    print(&quot;&quot;)    </code></pre>
<div class="parsed-literal">
<p>symboling False 205 Name: symboling, dtype: int64</p>
<p>normalized-losses False 164 True 41 Name: normalized-losses, dtype: int64</p>
<p>make False 205 Name: make, dtype: int64</p>
<p>fuel-type False 205 Name: fuel-type, dtype: int64</p>
<p>aspiration False 205 Name: aspiration, dtype: int64</p>
<p>num-of-doors False 203 True 2 Name: num-of-doors, dtype: int64</p>
<p>body-style False 205 Name: body-style, dtype: int64</p>
<p>drive-wheels False 205 Name: drive-wheels, dtype: int64</p>
<p>engine-location False 205 Name: engine-location, dtype: int64</p>
<p>wheel-base False 205 Name: wheel-base, dtype: int64</p>
<p>length False 205 Name: length, dtype: int64</p>
<p>width False 205 Name: width, dtype: int64</p>
<p>height False 205 Name: height, dtype: int64</p>
<p>curb-weight False 205 Name: curb-weight, dtype: int64</p>
<p>engine-type False 205 Name: engine-type, dtype: int64</p>
<p>num-of-cylinders False 205 Name: num-of-cylinders, dtype: int64</p>
<p>engine-size False 205 Name: engine-size, dtype: int64</p>
<p>fuel-system False 205 Name: fuel-system, dtype: int64</p>
<p>bore False 201 True 4 Name: bore, dtype: int64</p>
<p>stroke False 201 True 4 Name: stroke, dtype: int64</p>
<p>compression-ratio False 205 Name: compression-ratio, dtype: int64</p>
<p>horsepower False 203 True 2 Name: horsepower, dtype: int64</p>
<p>peak-rpm False 203 True 2 Name: peak-rpm, dtype: int64</p>
<p>city-mpg False 205 Name: city-mpg, dtype: int64</p>
<p>highway-mpg False 205 Name: highway-mpg, dtype: int64</p>
<p>price False 201 True 4 Name: price, dtype: int64</p>
</div>
<div class="container alert alert-block alert-info">
<p>Based on the summary above, each column has 205 rows of data, seven columns containing missing data:</p>
<ol>
<li>
<p>“normalized-losses”: 41 missing data</p>
</li>
<li>
<p>“num-of-doors”: 2 missing data</p>
</li>
<li>
<p>“bore”: 4 missing data</p>
</li>
<li>
<p>“stroke” : 4 missing data</p>
</li>
<li>
<p>“horsepower”: 2 missing data</p>
</li>
<li>
<p>“peak-rpm”: 2 missing data</p>
</li>
<li>
<p>“price”: 4 missing data</p>
</li>
</ol>
</div>
<h3 id="deal-with-missing-data"><code>Deal with missing data</code></h3>
<div class="alert alert-danger alertdanger" style="margin-top: 20px">
<p>How to deal with missing data?</p>
<ol>
<li>
<p>drop data a. drop the whole row b. drop the whole column</p>
</li>
<li>
<p>replace data a. replace it by mean b. replace it by frequency c. replace it based on other functions</p>
</li>
<p>Whole columns should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty enough to drop entirely. We have some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method to many different columns:</p>
<p>Replace by mean:</p>
<ul>
<li>
<p>“normalized-losses”: 41 missing data, replace them with mean</p>
</li>
<li>
<p>“stroke”: 4 missing data, replace them with mean</p>
</li>
<li>
<p>“bore”: 4 missing data, replace them with mean</p>
</li>
<li>
<p>“horsepower”: 2 missing data, replace them with mean</p>
</li>
<li>
<p>“peak-rpm”: 2 missing data, replace them with mean</p>
</li>
</ul>
<p>Replace by frequency:</p>
<ul>
<li>
<p>“num-of-doors”: 2 missing data, replace them with “four”.</p>
<ul>
<li>
<p>Reason: 84% sedans is four doors. Since four doors is most frequent, it is most likely to occur</p>
</li>
</ul>
<pre><code>&lt;/li&gt;</code></pre>
</ul>
<p>Drop the whole row:</p>
<ul>
<li>
<p>“price”: 4 missing data, simply delete the whole row</p>
<ul>
<li>
<p>Reason: price is what we want to predict. Any data entry without price data cannot be used for prediction; therefore any row now without price data is not useful to us</p>
</li>
</ul>
<pre><code>&lt;/li&gt;</code></pre>
</ul>
</ol>
</div>
<div class="container alert alert-danger alertdanger">
<p>Replace by mean:</p>
</div>
<div class="container alert alert-block alert-info">
<p>“normalized-losses”: 41 missing data, replace them with mean</p>
</div>
<pre class="ipython3"><code># Calculate the average of the column
avg_norm_loss = df[&quot;normalized-losses&quot;].astype(&quot;float&quot;).mean(axis=0)
print(&quot;Average of normalized-losses:&quot;, avg_norm_loss)

#Replace &quot;NaN&quot; by mean value in &quot;normalized-losses&quot; column
df[&quot;normalized-losses&quot;].replace(np.nan, avg_norm_loss, inplace=True)</code></pre>
<div class="parsed-literal">
<p>Average of normalized-losses: 122.0</p>
</div>
<div class="container alert alert-block alert-info">
<p>“bore”: 4 missing data, replace them with mean</p>
</div>
<pre class="ipython3"><code>#Calculate the mean value for &#39;bore&#39; column
avg_bore=df[&#39;bore&#39;].astype(&#39;float&#39;).mean(axis=0)
print(&quot;Average of bore:&quot;, avg_bore)

#Replace NaN by mean value
df[&quot;bore&quot;].replace(np.nan, avg_bore, inplace=True)</code></pre>
<div class="parsed-literal">
<p>Average of bore: 3.3297512437810943</p>
</div>
<div class="container alert alert-block alert-info">
<p>“stroke”: 4 missing data, replace them with mean</p>
</div>
<pre class="ipython3"><code>#Calculate the mean vaule for &quot;stroke&quot; column
avg_stroke = df[&quot;stroke&quot;].astype(&quot;float&quot;).mean(axis = 0)
print(&quot;Average of stroke:&quot;, avg_stroke)

#Replace &quot;stroke&quot; by mean value
df[&quot;stroke&quot;].replace(np.nan, avg_stroke, inplace = True)</code></pre>
<div class="parsed-literal">
<p>Average of stroke: 3.255422885572139</p>
</div>
<div class="container alert alert-block alert-info">
<p>“peak-rpm”: 2 missing data, replace them with mean</p>
</div>
<pre class="ipython3"><code>#Calculate the mean vaule for &quot;peak-rpm&quot; column
avg_peak_rpm = df[&quot;peak-rpm&quot;].astype(&quot;float&quot;).mean(axis = 0)
print(&quot;Average of peak-rpm:&quot;, avg_peak_rpm)

#Replace &quot;peak-rpm&quot; by mean value
df[&quot;peak-rpm&quot;].replace(np.nan, avg_peak_rpm, inplace = True)</code></pre>
<div class="parsed-literal">
<p>Average of peak-rpm: 5125.369458128079</p>
</div>
<div class="container alert alert-block alert-info">
<p>“horsepower”: 2 missing data, replace them with mean</p>
</div>
<pre class="ipython3"><code>#Calculate the mean vaule for &quot;horsepower&quot; column
avg_horsepower = df[&#39;horsepower&#39;].astype(&#39;float&#39;).mean(axis=0)
print(&quot;Average horsepower:&quot;, avg_horsepower)

#Replace &quot;horsepower&quot; by mean value
df[&#39;horsepower&#39;].replace(np.nan, avg_horsepower, inplace=True)</code></pre>
<div class="parsed-literal">
<p>Average horsepower: 104.25615763546799</p>
</div>
<div class="container alert alert-danger alertdanger">
<p>Replace by frequency:</p>
</div>
<div class="container alert alert-block alert-info">
<p>“num-of-doors”: 2 missing data</p>
</div>
<p><strong>To see which values are present in a particular column, we can use the ``.value_counts()`` method:</strong></p>
<pre class="ipython3"><code>df[&#39;num-of-doors&#39;].value_counts()</code></pre>
<div class="parsed-literal">
<p>four 114 two 89 Name: num-of-doors, dtype: int64</p>
</div>
<p><strong>We can see that four doors are the most common type. We can also use the ``.idxmax()`` method to calculate for us the most common type automatically:</strong></p>
<pre class="ipython3"><code>df[&#39;num-of-doors&#39;].value_counts().idxmax()</code></pre>
<div class="parsed-literal">
<p>'four'</p>
</div>
<p><strong>The replacement procedure is very similar to what we have seen previously</strong></p>
<pre class="ipython3"><code>#replace the missing &#39;num-of-doors&#39; values by the most frequent 
df[&quot;num-of-doors&quot;].replace(np.nan, &quot;four&quot;, inplace=True)</code></pre>
<div class="container alert alert-block alert-info">
<p>Finally, let’s drop all rows that do not have price data:</p>
</div>
<pre class="ipython3"><code># simply drop whole row with NaN in &quot;price&quot; column
df.dropna(subset=[&quot;price&quot;], axis=0, inplace=True)

# reset index, because we droped two rows
df.reset_index(drop=True, inplace=True)</code></pre>
<pre class="ipython3"><code>df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>fuel-type</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>...</th>
      <th>engine-size</th>
      <th>fuel-system</th>
      <th>bore</th>
      <th>stroke</th>
      <th>compression-ratio</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>122.0</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>13495</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>122.0</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>130</td>
      <td>mpfi</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000</td>
      <td>21</td>
      <td>27</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>122.0</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>...</td>
      <td>152</td>
      <td>mpfi</td>
      <td>2.68</td>
      <td>3.47</td>
      <td>9.0</td>
      <td>154</td>
      <td>5000</td>
      <td>19</td>
      <td>26</td>
      <td>16500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>109</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>10.0</td>
      <td>102</td>
      <td>5500</td>
      <td>24</td>
      <td>30</td>
      <td>13950</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>...</td>
      <td>136</td>
      <td>mpfi</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.0</td>
      <td>115</td>
      <td>5500</td>
      <td>18</td>
      <td>22</td>
      <td>17450</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 26 columns</p>
</div>
<h3 id="correct_data_format">
<p>Correct data format</p>
</h3>
<div class="container alert alert-danger alertdanger">
<p>We are almost there!</p>
<p>
<p>The last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other).</p>
</p>
<h3>
<p>Data Types</p>
</h3>
<div class="container alert alert-danger alertdanger">
<p>
<p>Data has a variety of types. The main types stored in Pandas dataframes are object, float, int, bool and datetime64. In order to better learn about each attribute, it is always good for us to know the data type of each column. In Pandas:</p>
</p>
</div>
<p>In Pandas, we use</p>
<p>
<p>.dtype() to check the data type</p>
</p>
<p>
<p>.astype() to change the data type</p>
</p>
</div>
<div class="container alert alert-block alert-info">
<p>Lets list the data types for each column</p>
</div>
<pre class="ipython3"><code>df.dtypes</code></pre>
<div class="parsed-literal">
<p>symboling int64 normalized-losses object make object fuel-type object aspiration object num-of-doors object body-style object drive-wheels object engine-location object wheel-base float64 length float64 width float64 height float64 curb-weight int64 engine-type object num-of-cylinders object engine-size int64 fuel-system object bore object stroke object compression-ratio float64 horsepower object peak-rpm object city-mpg int64 highway-mpg int64 price object dtype: object</p>
</div>
<div class="container alert alert-block alert-info">
<p>
<p>As we can see above, some columns are not of the correct data type. Numerical variables should have type ‘float’ or ‘int’, and variables with strings such as categories should have type ‘object’. For example, ‘bore’ and ‘stroke’ variables are numerical values that describe the engines, so we should expect them to be of the type ‘float’ or ‘int’; however, they are shown as type ‘object’. We have to convert data types into a proper format for each column using the astype() method.</p>
</p>
</div>
<pre class="ipython3"><code>#Convert data types to proper format
df[[&quot;bore&quot;, &quot;stroke&quot;,&quot;price&quot;,&quot;peak-rpm&quot;]] = df[[&quot;bore&quot;, &quot;stroke&quot;,&quot;price&quot;,&quot;peak-rpm&quot;]].astype(&quot;float&quot;)
df[[&quot;normalized-losses&quot;]] = df[[&quot;normalized-losses&quot;]].astype(&quot;int&quot;)</code></pre>
<p><strong>Let us list the columns after the conversion</strong></p>
<pre class="ipython3"><code>df.info(verbose = False)</code></pre>
<div class="parsed-literal">
<p>&lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 201 entries, 0 to 200 Columns: 26 entries, symboling to price dtypes: float64(9), int32(1), int64(5), object(11) memory usage: 40.2+ KB</p>
</div>
<pre class="ipython3"><code>df.info()</code></pre>
<div class="parsed-literal">
<p>&lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 201 entries, 0 to 200 Data columns (total 26 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 symboling 201 non-null int64 1 normalized-losses 201 non-null int32 2 make 201 non-null object 3 fuel-type 201 non-null object 4 aspiration 201 non-null object 5 num-of-doors 201 non-null object 6 body-style 201 non-null object 7 drive-wheels 201 non-null object 8 engine-location 201 non-null object 9 wheel-base 201 non-null float64 10 length 201 non-null float64 11 width 201 non-null float64 12 height 201 non-null float64 13 curb-weight 201 non-null int64 14 engine-type 201 non-null object 15 num-of-cylinders 201 non-null object 16 engine-size 201 non-null int64 17 fuel-system 201 non-null object 18 bore 201 non-null float64 19 stroke 201 non-null float64 20 compression-ratio 201 non-null float64 21 horsepower 201 non-null object 22 peak-rpm 201 non-null float64 23 city-mpg 201 non-null int64 24 highway-mpg 201 non-null int64 25 price 201 non-null float64 dtypes: float64(9), int32(1), int64(5), object(11) memory usage: 40.2+ KB</p>
</div>
<div class="container alert alert-block alert-info">
<p>Wonderful!</p>
<p>Now, we finally obtain the cleaned dataset with no missing values and all data in its proper format.</p>
</div>
<h2 id="data_standardization">
<p>Data Standardization</p>
</h2>
<div class="alert alert-danger alertdanger" style="margin-top: 20px">
<p>
<p>Data is usually collected from different agencies with different formats. (Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)</p>
</p>
<p>What is Standardization?</p>
<p>
<p>Standardization is the process of transforming data into a common format which allows the researcher to make the meaningful comparison.</p>
</p>
<p>Example</p>
<p>
<p>Transform mpg to L/100km:</p>
</p>
<p>
<p>In our dataset, the fuel consumption columns “city-mpg” and “highway-mpg” are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accept the fuel consumption with L/100km standard</p>
</p>
<p>
<p>We will need to apply data transformation to transform mpg into L/100km?</p>
</p>
<p>
<p>The formula for unit conversion is</p>
<p>
<p>L/100km = 235 / mpg</p>
<p>
<p>We can do many mathematical operations directly in Pandas.</p>
</p>
</div>
<pre class="ipython3"><code># Convert mpg to L/100km by mathematical operation (235 divided by mpg)
df[&#39;city-L/100km&#39;] = 235/df[&quot;city-mpg&quot;]
df[&quot;highway-L/100km&quot;] = 235/df[&quot;highway-mpg&quot;]

# check your transformed data 
df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>fuel-type</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>...</th>
      <th>bore</th>
      <th>stroke</th>
      <th>compression-ratio</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000.0</td>
      <td>21</td>
      <td>27</td>
      <td>13495.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>...</td>
      <td>3.47</td>
      <td>2.68</td>
      <td>9.0</td>
      <td>111</td>
      <td>5000.0</td>
      <td>21</td>
      <td>27</td>
      <td>16500.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>gas</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>...</td>
      <td>2.68</td>
      <td>3.47</td>
      <td>9.0</td>
      <td>154</td>
      <td>5000.0</td>
      <td>19</td>
      <td>26</td>
      <td>16500.0</td>
      <td>12.368421</td>
      <td>9.038462</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>...</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>10.0</td>
      <td>102</td>
      <td>5500.0</td>
      <td>24</td>
      <td>30</td>
      <td>13950.0</td>
      <td>9.791667</td>
      <td>7.833333</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>gas</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>...</td>
      <td>3.19</td>
      <td>3.40</td>
      <td>8.0</td>
      <td>115</td>
      <td>5500.0</td>
      <td>18</td>
      <td>22</td>
      <td>17450.0</td>
      <td>13.055556</td>
      <td>10.681818</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 28 columns</p>
</div>
<h2 id="data_normalization">
<p>Data Normalization</p>
</h2>
<div class="container alert alert-danger alertdanger">
<p>Why normalization?</p>
<p>
<p>Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling variable so the variable values range from 0 to 1</p>
</p>
<p>Example</p>
<p>
<p>To demonstrate normalization, let’s say we want to scale the columns “length”, “width” and “height”</p>
</p>
<p>
<p>Target:would like to Normalize those variables so their value ranges from 0 to 1.</p>
</p>
<p>
<p>Approach: replace original value by (original value)/(maximum value)</p>
</p>
</div>
<pre class="ipython3"><code>df[[&quot;length&quot;,&quot;width&quot;,&quot;height&quot;]].head() # these values vary highly w.r.t rest column values</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>width</th>
      <th>height</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>168.8</td>
      <td>64.1</td>
      <td>48.8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>168.8</td>
      <td>64.1</td>
      <td>48.8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>171.2</td>
      <td>65.5</td>
      <td>52.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>176.6</td>
      <td>66.2</td>
      <td>54.3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>176.6</td>
      <td>66.4</td>
      <td>54.3</td>
    </tr>
  </tbody>
</table>
</div>
<pre class="ipython3"><code># replace (original value) by (original value)/(maximum value) -&gt; Simple Feature Scaling
df[&#39;length&#39;] = df[&#39;length&#39;]/df[&#39;length&#39;].max()
df[&#39;width&#39;] = df[&#39;width&#39;]/df[&#39;width&#39;].max()
df[&#39;height&#39;] = df[&#39;height&#39;]/df[&#39;height&#39;].max()

# show the scaled columns
df[[&quot;length&quot;,&quot;width&quot;,&quot;height&quot;]].head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>width</th>
      <th>height</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.811148</td>
      <td>0.890278</td>
      <td>0.816054</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.811148</td>
      <td>0.890278</td>
      <td>0.816054</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.822681</td>
      <td>0.909722</td>
      <td>0.876254</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.848630</td>
      <td>0.919444</td>
      <td>0.908027</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.848630</td>
      <td>0.922222</td>
      <td>0.908027</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="binning">
<p>Binning</p>
</h2>
<div class="container alert alert-danger alertdanger">
<p>Why binning?</p>
<p>
<p>Binning is a process of transforming continuous numerical variables into discrete categorical ‘bins’, for grouped analysis.</p>
</p>
<p>Example:</p>
<p>
<p>In our dataset, “horsepower” is a real valued variable ranging from 48 to 288, it has 57 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three ‘bins’ to simplify analysis?</p>
</p>
<p>
<p>We will use the Pandas method ‘cut’ to segment the ‘horsepower’ column into 3 bins</p>
</p>
</div>
<h3>
<p>Example of Binning Data In Pandas</p>
</h3>
<p>Convert data to correct format</p>
<p>Lets plot the histogram of horspower, to see what the distribution of horsepower looks like.</p>
<pre class="ipython3"><code>df[&quot;horsepower&quot;]=df[&quot;horsepower&quot;].astype(int, copy=True)
plt.hist(df[&quot;horsepower&quot;])

# set x/y labels and plot title
plt.xlabel(&quot;horsepower&quot;)
plt.ylabel(&quot;count&quot;)
plt.title(&quot;horsepower bins&quot;)</code></pre>
<div class="parsed-literal">
<p>Text(0.5, 1.0, 'horsepower bins')</p>
</div>
<p><img src="output_59_1.png" alt="image" /></p>
<div class="container alert alert-danger alertdanger">
<p>
<p>We would like 3 bins of equal size bandwidth so we use numpy’s linspace(start_value, end_value, numbers_generated) function.</p>
</p>
<p>
<p>Since we want to include the minimum value of horsepower we want to set start_value=min(df[“horsepower”]).</p>
</p>
<p>
<p>Since we want to include the maximum value of horsepower we want to set end_value=max(df[“horsepower”]).</p>
</p>
<p>
<p>Since we are building 3 bins of equal length, there should be 4 dividers, so numbers_generated=4.</p>
</p>
</div>
<p><strong>We build a bin array, with a minimum value to a maximum value, with bandwidth calculated above. The bins will be values used to determine when one bin ends and another begins.</strong></p>
<pre class="ipython3"><code>bins = np.linspace(min(df[&quot;horsepower&quot;]), max(df[&quot;horsepower&quot;]), 4)
bins</code></pre>
<div class="parsed-literal">
<p>array([ 48. , 119.33333333, 190.66666667, 262. ])</p>
</div>
<p><strong>We set group names:</strong></p>
<pre class="ipython3"><code>group_names = [&#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;]</code></pre>
<p><strong>We apply the function ``cut`` that determine what each value of ``df['horsepower']`` belongs to.</strong></p>
<pre class="ipython3"><code>df[&#39;horsepower-binned&#39;] = pd.cut(df[&#39;horsepower&#39;], bins, labels=group_names, include_lowest=True )

#Lets see the number of vehicles in each bin
df[&quot;horsepower-binned&quot;].value_counts()</code></pre>
<div class="parsed-literal">
<p>Low 153 Medium 43 High 5 Name: horsepower-binned, dtype: int64</p>
</div>
<p><strong>Lets plot the distribution of each bin.</strong></p>
<pre class="ipython3"><code>pyplot.bar(group_names, df[&quot;horsepower-binned&quot;].value_counts())

# set x/y labels and plot title
plt.xlabel(&quot;horsepower&quot;)
plt.ylabel(&quot;count&quot;)
plt.title(&quot;horsepower bins&quot;)</code></pre>
<div class="parsed-literal">
<p>Text(0.5, 1.0, 'horsepower bins')</p>
</div>
<p><img src="output_68_1.png" alt="image" /></p>
<p><strong>Check the dataframe above carefully, you will find the last column provides the bins for “horsepower” with 3 categories (“Low”,“Medium” and “High”)</strong></p>
<h3>
<p>Bins visualization</p>
</h3>
<p>Normally, a histogram is used to visualize the distribution of bins we created above.</p>
<pre class="ipython3"><code># draw historgram of attribute &quot;horsepower&quot; with bins = 3
plt.hist(df[&quot;horsepower&quot;], bins = 3)

# set x/y labels and plot title
plt.xlabel(&quot;horsepower&quot;)
plt.ylabel(&quot;count&quot;)
plt.title(&quot;horsepower bins&quot;)</code></pre>
<div class="parsed-literal">
<p>Text(0.5, 1.0, 'horsepower bins')</p>
</div>
<p><img src="output_70_1.png" alt="image" /></p>
<h2 id="indicator">
<p>Indicator variable (or dummy variable)</p>
</h2>
<div class="container alert alert-danger alertdanger">
<p>What is an indicator variable?</p>
<p>
<p>An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called ‘dummies’ because the numbers themselves don’t have inherent meaning.</p>
</p>
<p>Why we use indicator variables?</p>
<p>
<p>So we can use categorical variables for regression analysis in the later modules.</p>
</p>
<p>Example</p>
<p>
<p>We see the column “fuel-type” has two unique values, “gas” or “diesel”. Regression doesn’t understand words, only numbers. To use this attribute in regression analysis, we convert “fuel-type” into indicator variables.</p>
</p>
<p>
<p>We will use the panda’s method ‘get_dummies’ to assign numerical values to different categories of fuel type.</p>
</p>
</div>
<p><strong>get indicator variables as ``fuel-type`` and assign it to data frame ``dummy_variable_1``</strong></p>
<pre class="ipython3"><code>dummy_variable_1 = pd.get_dummies(df[&quot;fuel-type&quot;])
dummy_variable_1.sample(5)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diesel</th>
      <th>gas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>135</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>65</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>change column names for clarity</strong></p>
<pre class="ipython3"><code>dummy_variable_1.rename(columns={&#39;gas&#39;:&#39;fuel-type-gas&#39;, &#39;diesel&#39;:&#39;fuel-type-diesel&#39;}, inplace=True)
dummy_variable_1.head(5)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>In the dataframe, column fuel-type has a value for ‘gas’ and ’diesel’as 0s and 1s now</strong></p>
<pre class="ipython3"><code># merge data frame &quot;df&quot; and &quot;dummy_variable_1&quot; 
df = pd.concat([df, dummy_variable_1], axis=1)

# drop original column &quot;fuel-type&quot; from &quot;df&quot;
df.drop(&quot;fuel-type&quot;, axis = 1, inplace=True)</code></pre>
<pre class="ipython3"><code>df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>aspiration</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>length</th>
      <th>...</th>
      <th>horsepower</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
      <th>horsepower-binned</th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>0.811148</td>
      <td>...</td>
      <td>111</td>
      <td>5000.0</td>
      <td>21</td>
      <td>27</td>
      <td>13495.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>std</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>0.811148</td>
      <td>...</td>
      <td>111</td>
      <td>5000.0</td>
      <td>21</td>
      <td>27</td>
      <td>16500.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>std</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>0.822681</td>
      <td>...</td>
      <td>154</td>
      <td>5000.0</td>
      <td>19</td>
      <td>26</td>
      <td>16500.0</td>
      <td>12.368421</td>
      <td>9.038462</td>
      <td>Medium</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>0.848630</td>
      <td>...</td>
      <td>102</td>
      <td>5500.0</td>
      <td>24</td>
      <td>30</td>
      <td>13950.0</td>
      <td>9.791667</td>
      <td>7.833333</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>std</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>0.848630</td>
      <td>...</td>
      <td>115</td>
      <td>5500.0</td>
      <td>18</td>
      <td>22</td>
      <td>17450.0</td>
      <td>13.055556</td>
      <td>10.681818</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div>
<p><strong>Repeat for ``aspiration`` column</strong></p>
<pre class="ipython3"><code># get indicator variables of aspiration and assign it to data frame &quot;dummy_variable_2&quot;
dummy_variable_2 = pd.get_dummies(df[&#39;aspiration&#39;])

# change column names for clarity
dummy_variable_2.rename(columns={&#39;std&#39;:&#39;aspiration-std&#39;, &#39;turbo&#39;: &#39;aspiration-turbo&#39;}, inplace=True)

# show first 5 instances of data frame &quot;dummy_variable_1&quot;
dummy_variable_2.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>aspiration-std</th>
      <th>aspiration-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<pre class="ipython3"><code># merge data frame &quot;df&quot; and &quot;dummy_variable_1&quot; 
df = pd.concat([df, dummy_variable_2], axis=1)

# drop original column &quot;fuel-type&quot; from &quot;df&quot;
df.drop(&quot;aspiration&quot;, axis = 1, inplace=True)
df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>length</th>
      <th>width</th>
      <th>...</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
      <th>horsepower-binned</th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
      <th>aspiration-std</th>
      <th>aspiration-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>0.811148</td>
      <td>0.890278</td>
      <td>...</td>
      <td>21</td>
      <td>27</td>
      <td>13495.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>two</td>
      <td>convertible</td>
      <td>rwd</td>
      <td>front</td>
      <td>88.6</td>
      <td>0.811148</td>
      <td>0.890278</td>
      <td>...</td>
      <td>21</td>
      <td>27</td>
      <td>16500.0</td>
      <td>11.190476</td>
      <td>8.703704</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>122</td>
      <td>alfa-romero</td>
      <td>two</td>
      <td>hatchback</td>
      <td>rwd</td>
      <td>front</td>
      <td>94.5</td>
      <td>0.822681</td>
      <td>0.909722</td>
      <td>...</td>
      <td>19</td>
      <td>26</td>
      <td>16500.0</td>
      <td>12.368421</td>
      <td>9.038462</td>
      <td>Medium</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>99.8</td>
      <td>0.848630</td>
      <td>0.919444</td>
      <td>...</td>
      <td>24</td>
      <td>30</td>
      <td>13950.0</td>
      <td>9.791667</td>
      <td>7.833333</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>164</td>
      <td>audi</td>
      <td>four</td>
      <td>sedan</td>
      <td>4wd</td>
      <td>front</td>
      <td>99.4</td>
      <td>0.848630</td>
      <td>0.922222</td>
      <td>...</td>
      <td>18</td>
      <td>22</td>
      <td>17450.0</td>
      <td>13.055556</td>
      <td>10.681818</td>
      <td>Low</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div>
<hr />
<h1 id="analyzing-individual-feature-patterns-using-visualization"><code>Analyzing Individual Feature Patterns using Visualization</code></h1>
<div class="container alert alert-body alert-info">
<p>How to choose the right visualization method? When visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.</p>
</div>
<pre class="ipython3"><code># list the data types for each column
print(df.dtypes)</code></pre>
<div class="parsed-literal">
<p>symboling int64 normalized-losses int32 make object num-of-doors object body-style object drive-wheels object engine-location object wheel-base float64 length float64 width float64 height float64 curb-weight int64 engine-type object num-of-cylinders object engine-size int64 fuel-system object bore float64 stroke float64 compression-ratio float64 horsepower int32 peak-rpm float64 city-mpg int64 highway-mpg int64 price float64 city-L/100km float64 highway-L/100km float64 horsepower-binned category fuel-type-diesel uint8 fuel-type-gas uint8 aspiration-std uint8 aspiration-turbo uint8 dtype: object</p>
</div>
<h2>
<ol type="1">
<li>Continuous numerical variables:</li>
</ol>
</h2>
<div class="container alert alert-danger alertdanger">
<p>
<p>Continuous numerical variables are variables that may contain any value within some range. Continuous numerical variables can have the type int64 or float64. A great way to visualize these variables is by using scatterplots with fitted lines.</p>
</p>
<p>
<p>In order to start understanding the (linear) relationship between an individual variable and the price. We can do this by using “regplot”, which plots the scatterplot plus the fitted regression line for the data.</p>
</p>
</div>
<h4>
<p>Positive linear relationship</p>
</h4>
<div class="container alert alert-body alert-info">
<p>Let’s find the scatterplot of “engine-size” and “price”</p>
</div>
<pre class="ipython3"><code>sns.regplot(x = &#39;engine-size&#39;, y = &#39;price&#39;, data = df)
plt.ylim(0,) # y axis starts from zero
plt.title(&quot;correlation between engine-size and price&quot;)
plt.show()</code></pre>
<p><img src="output_88_0.png" alt="image" /></p>
<p><strong>As the ``engine-size goes up, the price goes up: this indicates a positive direct correlation`` between these two variables. Engine size seems like a pretty good predictor of price since the regression line is almost a perfect diagonal line.We can examine the correlation between engine-size and price and see it’s approximately ``0.87``</strong></p>
<div class="container alert alert-body alert-info">
<p>we can calculate the correlation between variables of type int64 or float64 using the method corr: The diagonal elements are always one</p>
</div>
<pre class="ipython3"><code>df[[&quot;engine-size&quot;, &quot;price&quot;]].corr()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>engine-size</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>engine-size</th>
      <td>1.000000</td>
      <td>0.872335</td>
    </tr>
    <tr>
      <th>price</th>
      <td>0.872335</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-body alert-info">
<p>Highway mpg is a potential predictor variable of price</p>
</div>
<p><strong>``Highway MPG``: the average a car will get while driving on an open stretch of road without stopping or starting, typically at a higher speed. ``City MPG``: the score a car will get on average in city conditions, with stopping and starting at lower speeds.</strong></p>
<pre class="ipython3"><code>sns.regplot(x = &#39;highway-mpg&#39;, y = &#39;price&#39;,data = df)
print(&#39;correlation between highway-mpg and price &#39;)
df[[&#39;highway-mpg&#39;, &#39;price&#39;]].corr()</code></pre>
<div class="parsed-literal">
<p>correlation between highway-mpg and price</p>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>highway-mpg</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>highway-mpg</th>
      <td>1.000000</td>
      <td>-0.704692</td>
    </tr>
    <tr>
      <th>price</th>
      <td>-0.704692</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<p><img src="output_92_2.png" alt="image" /></p>
<p><strong>As the ``highway-mpg goes up, the price goes down: this indicates an inverse/negative relationship`` between these two variables. Highway mpg could potentially be a predictor of price.We can examine the correlation between highway-mpg and price and see it’s approximately ``-0.704``</strong></p>
<div class="container alert alert-danger alertdanger">
<p>Weak Linear Relationship</p>
</div>
<div class="container alert alert-body alert-info">
<p>Let’s see if Peak-rpm as a predictor variable of price</p>
</div>
<pre class="ipython3"><code>sns.regplot(x = &#39;peak-rpm&#39;, y = &#39;price&#39;, data = df)
print(&#39;correlation between peak-rpm and price &#39;)
df[[&#39;peak-rpm&#39;,&#39;price&#39;]].corr()</code></pre>
<div class="parsed-literal">
<p>correlation between peak-rpm and price</p>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>peak-rpm</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>peak-rpm</th>
      <td>1.000000</td>
      <td>-0.101616</td>
    </tr>
    <tr>
      <th>price</th>
      <td>-0.101616</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<p><img src="output_96_2.png" alt="image" /></p>
<p><strong>``Peak rpm does not seem like a good predictor of the price at all`` since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore it’s it is not a reliable variable.We can examine the ``correlation`` between peak-rp and price and see it’s approximately ``-0.101616``</strong></p>
<div class="container alert alert-body alert-info">
<p>Let’s see if stroke as a predictor variable of price</p>
</div>
<pre class="ipython3"><code>print(&#39;correlation between strike and prce :&#39;)

# correlation results between &quot;price&quot; and &quot;stroke&quot; do you expect a linear relationship?
sns.regplot(x = &#39;stroke&#39;, y = &#39;price&#39;, data = df)

# correlation between x=&quot;stroke&quot;, y=&quot;price&quot;.
df[[&#39;stroke&#39;,&#39;price&#39;]].corr()</code></pre>
<div class="parsed-literal">
<p>correlation between strike and prce :</p>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stroke</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>stroke</th>
      <td>1.000000</td>
      <td>0.082269</td>
    </tr>
    <tr>
      <th>price</th>
      <td>0.082269</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
<p><img src="output_99_2.png" alt="image" /></p>
<p><strong>``Stroke does not seem like a good predictor of the price at all`` since the regression line is close to horizontal. Also, the data points are very scattered and far from the fitted line, showing lots of variability. Therefore it’s it is not a reliable variable.We can examine the ``correlation`` between stroke and price and see it’s approximately ``0.08``</strong></p>
<h3>
<ol start="2" type="1">
<li>Categorical variables</li>
</ol>
</h3>
<div class="alert alert-danger alertdanger" style="margin-top: 20px">
<p>
<p>These are variables that describe a characteristic of a data unit, and are selected from a small group of categories. The categorical variables can have the type object or int64. A good way to visualize categorical variables is by using boxplots.</p>
</div>
<div class="container alert alert-body alert-info">
<p>Let’s look at the relationship between body-style and price.</p>
</div>
<pre class="ipython3"><code>sns.boxplot(x=&quot;body-style&quot;, y=&quot;price&quot;, data=df)</code></pre>
<div class="parsed-literal">
<p>&lt;AxesSubplot:xlabel='body-style', ylabel='price'&gt;</p>
</div>
<p><img src="output_103_1.png" alt="image" /></p>
<p><strong>We see that the distributions of price between the different body-style categories have a significant overlap, and so body-style would not be a good predictor of price.</strong></p>
<div class="container alert alert-body alert-info">
<p>Let’s examine if engine-location as a predictor variable of price :</p>
</div>
<pre class="ipython3"><code>sns.boxplot(x = &#39;engine-location&#39;, y = &#39;price&#39;, data = df)</code></pre>
<div class="parsed-literal">
<p>&lt;AxesSubplot:xlabel='engine-location', ylabel='price'&gt;</p>
</div>
<p><img src="output_105_1.png" alt="image" /></p>
<p><strong>Here we see that the distribution of price between these two ``engine-location`` categories, front and rear, are distinct enough to take engine-location as a ``potential good predictor of price``.</strong></p>
<div class="container alert alert-body alert-info">
<p>Let’s examine if drive-wheels as a predictor variable of price :</p>
</div>
<p><strong>A ``drive wheel`` is a wheel of a motor vehicle that transmits force, transforming torque into tractive force from the tires to the road, causing the vehicle to move.</strong></p>
<pre class="ipython3"><code>sns.boxplot(x = &#39;drive-wheels&#39;, y = &#39;price&#39;, data = df)</code></pre>
<div class="parsed-literal">
<p>&lt;AxesSubplot:xlabel='drive-wheels', ylabel='price'&gt;</p>
</div>
<p><img src="output_107_1.png" alt="image" /></p>
<p><strong>Here we see that the distribution of price between the different drive-wheels categories differs; as such ``drive-wheels could potentially be a predictor of price.``</strong></p>
<h3>
<ol start="3" type="1">
<li>Descriptive Statistical Analysis</li>
</ol>
</h3>
<div class="alert alert-danger alertdanger" style="margin-top: 20px">
<p>
<p>Let’s first take a look at the variables by utilizing a description method.</p>
</p>
<p>
<p>The describe function automatically computes basic statistics for all continuous variables. Any NaN values are automatically skipped in these statistics.</p>
</p>
<p>This will show:</p>
<ul>
<li>
<p>the count of that variable</p>
</li>
<li>
<p>the mean</p>
</li>
<li>
<p>the standard deviation (std)</p>
</li>
<li>
<p>the minimum value</p>
</li>
<li>
<p>the IQR (Interquartile Range: 25%, 50% and 75%)</p>
</li>
<li>
<p>the maximum value</p>
</li>
<ul>
</div>
<p>We can apply the method describe as follows:</p>
<pre class="ipython3"><code>df.describe()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>wheel-base</th>
      <th>length</th>
      <th>width</th>
      <th>height</th>
      <th>curb-weight</th>
      <th>engine-size</th>
      <th>bore</th>
      <th>stroke</th>
      <th>...</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
      <th>aspiration-std</th>
      <th>aspiration-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>201.000000</td>
      <td>201.00000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>...</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.840796</td>
      <td>122.00000</td>
      <td>98.797015</td>
      <td>0.837102</td>
      <td>0.915126</td>
      <td>0.899108</td>
      <td>2555.666667</td>
      <td>126.875622</td>
      <td>3.330692</td>
      <td>3.256874</td>
      <td>...</td>
      <td>5117.665368</td>
      <td>25.179104</td>
      <td>30.686567</td>
      <td>13207.129353</td>
      <td>9.944145</td>
      <td>8.044957</td>
      <td>0.099502</td>
      <td>0.900498</td>
      <td>0.820896</td>
      <td>0.179104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.254802</td>
      <td>31.99625</td>
      <td>6.066366</td>
      <td>0.059213</td>
      <td>0.029187</td>
      <td>0.040933</td>
      <td>517.296727</td>
      <td>41.546834</td>
      <td>0.268072</td>
      <td>0.316048</td>
      <td>...</td>
      <td>478.113805</td>
      <td>6.423220</td>
      <td>6.815150</td>
      <td>7947.066342</td>
      <td>2.534599</td>
      <td>1.840739</td>
      <td>0.300083</td>
      <td>0.300083</td>
      <td>0.384397</td>
      <td>0.384397</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-2.000000</td>
      <td>65.00000</td>
      <td>86.600000</td>
      <td>0.678039</td>
      <td>0.837500</td>
      <td>0.799331</td>
      <td>1488.000000</td>
      <td>61.000000</td>
      <td>2.540000</td>
      <td>2.070000</td>
      <td>...</td>
      <td>4150.000000</td>
      <td>13.000000</td>
      <td>16.000000</td>
      <td>5118.000000</td>
      <td>4.795918</td>
      <td>4.351852</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>101.00000</td>
      <td>94.500000</td>
      <td>0.801538</td>
      <td>0.890278</td>
      <td>0.869565</td>
      <td>2169.000000</td>
      <td>98.000000</td>
      <td>3.150000</td>
      <td>3.110000</td>
      <td>...</td>
      <td>4800.000000</td>
      <td>19.000000</td>
      <td>25.000000</td>
      <td>7775.000000</td>
      <td>7.833333</td>
      <td>6.911765</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>122.00000</td>
      <td>97.000000</td>
      <td>0.832292</td>
      <td>0.909722</td>
      <td>0.904682</td>
      <td>2414.000000</td>
      <td>120.000000</td>
      <td>3.310000</td>
      <td>3.290000</td>
      <td>...</td>
      <td>5125.369458</td>
      <td>24.000000</td>
      <td>30.000000</td>
      <td>10295.000000</td>
      <td>9.791667</td>
      <td>7.833333</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.000000</td>
      <td>137.00000</td>
      <td>102.400000</td>
      <td>0.881788</td>
      <td>0.925000</td>
      <td>0.928094</td>
      <td>2926.000000</td>
      <td>141.000000</td>
      <td>3.580000</td>
      <td>3.410000</td>
      <td>...</td>
      <td>5500.000000</td>
      <td>30.000000</td>
      <td>34.000000</td>
      <td>16500.000000</td>
      <td>12.368421</td>
      <td>9.400000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>3.000000</td>
      <td>256.00000</td>
      <td>120.900000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4066.000000</td>
      <td>326.000000</td>
      <td>3.940000</td>
      <td>4.170000</td>
      <td>...</td>
      <td>6600.000000</td>
      <td>49.000000</td>
      <td>54.000000</td>
      <td>45400.000000</td>
      <td>18.076923</td>
      <td>14.687500</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 22 columns</p>
</div>
<div class="container alert alert-body alert-info">
<p>The default setting of describe skips variables of type object. We can apply the method describe on the variables of type object as follows:</p>
</div>
<pre class="ipython3"><code>df.describe(include = [&#39;object&#39;])</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>make</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>engine-type</th>
      <th>num-of-cylinders</th>
      <th>fuel-system</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>22</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>2</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
    </tr>
    <tr>
      <th>top</th>
      <td>toyota</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>ohc</td>
      <td>four</td>
      <td>mpfi</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>32</td>
      <td>115</td>
      <td>94</td>
      <td>118</td>
      <td>198</td>
      <td>145</td>
      <td>157</td>
      <td>92</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-body alert-info">
<p>We can apply the method describe on the variables of type all datatypes as follows:</p>
</div>
<pre class="ipython3"><code>df.describe(include = &#39;all&#39;)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>make</th>
      <th>num-of-doors</th>
      <th>body-style</th>
      <th>drive-wheels</th>
      <th>engine-location</th>
      <th>wheel-base</th>
      <th>length</th>
      <th>width</th>
      <th>...</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
      <th>horsepower-binned</th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
      <th>aspiration-std</th>
      <th>aspiration-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>201.000000</td>
      <td>201.00000</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>...</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
      <td>201.000000</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>22</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>top</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>toyota</td>
      <td>four</td>
      <td>sedan</td>
      <td>fwd</td>
      <td>front</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Low</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>32</td>
      <td>115</td>
      <td>94</td>
      <td>118</td>
      <td>198</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>153</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.840796</td>
      <td>122.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>98.797015</td>
      <td>0.837102</td>
      <td>0.915126</td>
      <td>...</td>
      <td>25.179104</td>
      <td>30.686567</td>
      <td>13207.129353</td>
      <td>9.944145</td>
      <td>8.044957</td>
      <td>NaN</td>
      <td>0.099502</td>
      <td>0.900498</td>
      <td>0.820896</td>
      <td>0.179104</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.254802</td>
      <td>31.99625</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>6.066366</td>
      <td>0.059213</td>
      <td>0.029187</td>
      <td>...</td>
      <td>6.423220</td>
      <td>6.815150</td>
      <td>7947.066342</td>
      <td>2.534599</td>
      <td>1.840739</td>
      <td>NaN</td>
      <td>0.300083</td>
      <td>0.300083</td>
      <td>0.384397</td>
      <td>0.384397</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-2.000000</td>
      <td>65.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>86.600000</td>
      <td>0.678039</td>
      <td>0.837500</td>
      <td>...</td>
      <td>13.000000</td>
      <td>16.000000</td>
      <td>5118.000000</td>
      <td>4.795918</td>
      <td>4.351852</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>101.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>94.500000</td>
      <td>0.801538</td>
      <td>0.890278</td>
      <td>...</td>
      <td>19.000000</td>
      <td>25.000000</td>
      <td>7775.000000</td>
      <td>7.833333</td>
      <td>6.911765</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>122.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>97.000000</td>
      <td>0.832292</td>
      <td>0.909722</td>
      <td>...</td>
      <td>24.000000</td>
      <td>30.000000</td>
      <td>10295.000000</td>
      <td>9.791667</td>
      <td>7.833333</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.000000</td>
      <td>137.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>102.400000</td>
      <td>0.881788</td>
      <td>0.925000</td>
      <td>...</td>
      <td>30.000000</td>
      <td>34.000000</td>
      <td>16500.000000</td>
      <td>12.368421</td>
      <td>9.400000</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>3.000000</td>
      <td>256.00000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>120.900000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>49.000000</td>
      <td>54.000000</td>
      <td>45400.000000</td>
      <td>18.076923</td>
      <td>14.687500</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>11 rows × 31 columns</p>
</div>
<h3>
<p>Value Counts</p>
</h3>
<div class="container alert alert-danger alertdanger">
<p>Value-counts is a good way of understanding how many units of each characteristic/variable we have. We can apply the value_counts method on the column ‘drive-wheels’. Don’t forget the method value_counts only works on Pandas series, not Pandas Dataframes. As a result, we only include one bracket df[‘drive-wheels’] not two brackets df[[‘drive-wheels’]].</p>
</div>
<pre class="ipython3"><code>df[&#39;drive-wheels&#39;].value_counts()</code></pre>
<div class="parsed-literal">
<p>fwd 118 rwd 75 4wd 8 Name: drive-wheels, dtype: int64</p>
</div>
<p><strong>We can convert the series to a Dataframe as follows :</strong></p>
<pre class="ipython3"><code>df[&#39;drive-wheels&#39;].value_counts().to_frame()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drive-wheels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fwd</th>
      <td>118</td>
    </tr>
    <tr>
      <th>rwd</th>
      <td>75</td>
    </tr>
    <tr>
      <th>4wd</th>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>Let’s repeat the above steps but save the results to the dataframe ``drive_wheels_counts`` and rename the column ``drive-wheels`` to ``value_counts``.</strong></p>
<p><strong>A vehicle’s drive wheel is the wheel and tire assembly that actually pushes or pulls the vehicle down the road.The four different types of drivetrain are all-wheel-drive (AWD), front wheel drive (FWD), rear wheel drive (RWD), and 4WD (4 wheel drive). (</strong><a href="https://www.drivespark.com/off-beat/car-drivetrain-systems-explained-022723.html">more</a><strong>)</strong></p>
<pre class="ipython3"><code>drive_wheels_counts = df[&#39;drive-wheels&#39;].value_counts().to_frame()
drive_wheels_counts.rename(columns={&#39;drive-wheels&#39;: &#39;value_counts&#39;}, inplace=True)
drive_wheels_counts.index.name = &#39;drive-wheels&#39; # Now let&#39;s rename the index to &#39;drive-wheels&#39;:
drive_wheels_counts</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>value_counts</th>
    </tr>
    <tr>
      <th>drive-wheels</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fwd</th>
      <td>118</td>
    </tr>
    <tr>
      <th>rwd</th>
      <td>75</td>
    </tr>
    <tr>
      <th>4wd</th>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>We can repeat the above process for the variable ``engine-location``.</strong></p>
<pre class="ipython3"><code>engine_location_counts = df[&#39;engine-location&#39;].value_counts().to_frame()
engine_location_counts.rename(columns = {&#39;engine-location&#39; : &#39;value_counts&#39;}, inplace = True)
engine_location_counts.index.name = &#39;engine-location&#39;
engine_location_counts</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>value_counts</th>
    </tr>
    <tr>
      <th>engine-location</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>front</th>
      <td>198</td>
    </tr>
    <tr>
      <th>rear</th>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>Examining the value counts of the ``engine location`` would ``not be a good predictor`` variable for the price. This is because we only have three cars with a rear engine and 198 with an engine in the front, this ``result is skewed``. Thus, we are not able to draw any conclusions about the engine location.</strong></p>
<h3>
<p>4.Basics of Grouping</p>
</h3>
<div class="container alert alert-danger alertdanger">
<p>
<p>The groupby method groups data by different categories. The data is grouped based on one or several variables and analysis is performed on the individual groups.</p>
</p>
<pre><code>&lt;p&gt;For example, let&#39;s group by the variable &lt;code&gt;drive-wheels&lt;/code&gt;. We see that there are 3 different categories of drive wheels.&lt;/p&gt;</code></pre>
</div>
<p><strong>For example, let’s group by the variable ``drive-wheels``. We see that there are 3 different categories of drive wheels.</strong></p>
<pre class="ipython3"><code>df[&quot;drive-wheels&quot;].unique()</code></pre>
<div class="parsed-literal">
<p>array(['rwd', 'fwd', '4wd'], dtype=object)</p>
</div>
<div class="container alert alert-body alert-info">
<p>
<p>If we want to know, on average, which type of drive wheel is most valuable, we can group drive-wheels and then average them.</p>
</p>
<p>
<p>We can select the columns drive-wheels, body-style and price, then assign it to the variable df_group_one.</p>
</p>
<pre><code>We can then calculate the average price for each of the different categories of data.</code></pre>
</div>
<pre class="ipython3"><code>df_group_one = df[[&#39;drive-wheels&#39;, &#39;body-style&#39;, &#39;price&#39;]]
df_group_one = df_group_one.groupby([&#39;drive-wheels&#39;],as_index = False).mean() # if not passes as_index False it will make drive wheels as index
df_group_one</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drive-wheels</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4wd</td>
      <td>10241.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>fwd</td>
      <td>9244.779661</td>
    </tr>
    <tr>
      <th>2</th>
      <td>rwd</td>
      <td>19757.613333</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-body alert-info">
<pre><code>&lt;p&gt;From our data, it seems rear-wheel drive vehicles are, on average, the most expensive, while 4-wheel and front-wheel are approximately the same in price.&lt;/p&gt;</code></pre>
<p>
<p>You can also group with multiple variables. For example, let’s group by both drive-wheels and body-style. This groups the dataframe by the unique combinations drive-wheels and body-style. We can store the results in the variable ‘grouped_test1’.</p>
</p>
</div>
<pre class="ipython3"><code># grouping results
df_gptest = df[[&#39;drive-wheels&#39;,&#39;body-style&#39;,&#39;price&#39;]]
grouped_test1 = df_gptest.groupby([&#39;drive-wheels&#39;,&#39;body-style&#39;],as_index=False).mean()
grouped_test1</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drive-wheels</th>
      <th>body-style</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4wd</td>
      <td>hatchback</td>
      <td>7603.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4wd</td>
      <td>sedan</td>
      <td>12647.333333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4wd</td>
      <td>wagon</td>
      <td>9095.750000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fwd</td>
      <td>convertible</td>
      <td>11595.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>fwd</td>
      <td>hardtop</td>
      <td>8249.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>fwd</td>
      <td>hatchback</td>
      <td>8396.387755</td>
    </tr>
    <tr>
      <th>6</th>
      <td>fwd</td>
      <td>sedan</td>
      <td>9811.800000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>fwd</td>
      <td>wagon</td>
      <td>9997.333333</td>
    </tr>
    <tr>
      <th>8</th>
      <td>rwd</td>
      <td>convertible</td>
      <td>23949.600000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>rwd</td>
      <td>hardtop</td>
      <td>24202.714286</td>
    </tr>
    <tr>
      <th>10</th>
      <td>rwd</td>
      <td>hatchback</td>
      <td>14337.777778</td>
    </tr>
    <tr>
      <th>11</th>
      <td>rwd</td>
      <td>sedan</td>
      <td>21711.833333</td>
    </tr>
    <tr>
      <th>12</th>
      <td>rwd</td>
      <td>wagon</td>
      <td>16994.222222</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-body alert-info">
<pre><code>This grouped data is much easier to visualize when it is made into a pivot table. A pivot table is like an Excel spreadsheet, with one variable along the column and another along the row. We can convert the dataframe to a pivot table using the method &quot;pivot &quot; to create a pivot table from the groups.</code></pre>
<p>In this case, we will leave the drive-wheel variable as the rows of the table, and pivot body-style to become the columns of the table:</p>
</div>
<pre class="ipython3"><code>grouped_pivot = grouped_test1.pivot(index=&#39;drive-wheels&#39;,columns=&#39;body-style&#39;)
grouped_pivot</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="5" halign="left">price</th>
    </tr>
    <tr>
      <th>body-style</th>
      <th>convertible</th>
      <th>hardtop</th>
      <th>hatchback</th>
      <th>sedan</th>
      <th>wagon</th>
    </tr>
    <tr>
      <th>drive-wheels</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4wd</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>7603.000000</td>
      <td>12647.333333</td>
      <td>9095.750000</td>
    </tr>
    <tr>
      <th>fwd</th>
      <td>11595.0</td>
      <td>8249.000000</td>
      <td>8396.387755</td>
      <td>9811.800000</td>
      <td>9997.333333</td>
    </tr>
    <tr>
      <th>rwd</th>
      <td>23949.6</td>
      <td>24202.714286</td>
      <td>14337.777778</td>
      <td>21711.833333</td>
      <td>16994.222222</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>Often, we won’t have data for some of the pivot cells. We can fill these missing cells with the value 0, but any other value could potentially be used as well. It should be mentioned that missing data is quite a complex subject and is an entire course on its own. For simplicity, let’s assign them 0</strong></p>
<pre class="ipython3"><code>grouped_pivot = grouped_pivot.fillna(0) #fill missing values with 0
grouped_pivot</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="5" halign="left">price</th>
    </tr>
    <tr>
      <th>body-style</th>
      <th>convertible</th>
      <th>hardtop</th>
      <th>hatchback</th>
      <th>sedan</th>
      <th>wagon</th>
    </tr>
    <tr>
      <th>drive-wheels</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4wd</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>7603.000000</td>
      <td>12647.333333</td>
      <td>9095.750000</td>
    </tr>
    <tr>
      <th>fwd</th>
      <td>11595.0</td>
      <td>8249.000000</td>
      <td>8396.387755</td>
      <td>9811.800000</td>
      <td>9997.333333</td>
    </tr>
    <tr>
      <th>rwd</th>
      <td>23949.6</td>
      <td>24202.714286</td>
      <td>14337.777778</td>
      <td>21711.833333</td>
      <td>16994.222222</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-body alert-info">
<pre><code>Use the &lt;code&gt;groupby&lt;/code&gt; function to find the average &lt;code&gt;price&lt;/code&gt; of each car based on &lt;code&gt;body-style&lt;/code&gt; ?</code></pre>
</div>
<pre class="ipython3"><code>df_gptest2 = df[[&#39;body-style&#39;,&#39;price&#39;]]
grouped_test_bodystyle = df_gptest2.groupby([&#39;body-style&#39;],as_index= False).mean()
grouped_test_bodystyle</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>body-style</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>convertible</td>
      <td>21890.500000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>hardtop</td>
      <td>22208.500000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>hatchback</td>
      <td>9957.441176</td>
    </tr>
    <tr>
      <th>3</th>
      <td>sedan</td>
      <td>14459.755319</td>
    </tr>
    <tr>
      <th>4</th>
      <td>wagon</td>
      <td>12371.960000</td>
    </tr>
  </tbody>
</table>
</div>
<div class="container alert alert-danger alertdanger">
<h4>
<p>Variables: Drive Wheels and Body Style vs Price</p>
</h4>
</div>
<div class="container alert alert-body alert-info">
<pre><code>Let&#39;s use a heat map to visualize the relationship between Body Style vs Price.</code></pre>
</div>
<pre class="ipython3"><code>plt.pcolor(grouped_pivot, cmap = &#39;RdBu&#39;)
plt.colorbar() # show vertical range
plt.show()</code></pre>
<p><img src="output_139_0.png" alt="image" /></p>
<div class="container alert alert-body alert-info">
<p>The heatmap plots the target variable (price) proportional to colour with respect to the variables ‘drive-wheel’ and ‘body-style’ in the vertical and horizontal axis respectively. This allows us to visualize how the price is related to ‘drive-wheel’ and ‘body-style’.</p>
<p>The default labels convey no useful information to us. Let’s change that:</p>
</div>
<pre class="ipython3"><code>grouped_pivot.index</code></pre>
<div class="parsed-literal">
<p>Index(['4wd', 'fwd', 'rwd'], dtype='object', name='drive-wheels')</p>
</div>
<pre class="ipython3"><code>fig, ax = plt.subplots()
im = ax.pcolor(grouped_pivot, cmap=&#39;RdBu&#39;)

#label names
row_labels = grouped_pivot.columns.levels[1] # accesing &quot;body-type&quot; from grouped.pivot
col_labels = grouped_pivot.index # # accesing &quot;drive-wheels&quot; from grouped.pivot

#move ticks and labels to the center
ax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)
ax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)

#insert labels
ax.set_xticklabels(row_labels, minor=False)
ax.set_yticklabels(col_labels, minor=False)

#rotate label if too long
plt.xticks(rotation=90)

fig.colorbar(im)
plt.show()</code></pre>
<p><img src="output_142_0.png" alt="image" /></p>
<h3 id="correlation-and-causation"><code>5. Correlation and Causation</code></h3>
<div class="container alert alert-danger alertdanger">
<p>
<p>Correlation: a measure of the extent of interdependence between variables.</p>
</p>
<p>
<p>Causation: the relationship between cause and effect between two variables.</p>
</p>
<p>
<p>It is important to know the difference between these two and that correlation does not imply causation. Determining correlation is much simpler the determining causation as causation may require independent experimentation.</p>
</p>
<p>Pearson Correlation</p>
</p>
<p>
<p>The Pearson Correlation measures the linear dependence between two variables X and Y.</p>
</p>
<p>
<p>The resulting coefficient is a value between -1 and 1 inclusive, where:</p>
</p>
<ul>
<li>
<p>1: Total positive linear correlation.</p>
</li>
<li>
<p>0: No linear correlation, the two variables most likely do not affect each other.</p>
</li>
<li>
<p>-1: Total negative linear correlation.</p>
</li>
</ul>
</div>
<pre class="ipython3"><code>df.corr()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>symboling</th>
      <th>normalized-losses</th>
      <th>wheel-base</th>
      <th>length</th>
      <th>width</th>
      <th>height</th>
      <th>curb-weight</th>
      <th>engine-size</th>
      <th>bore</th>
      <th>stroke</th>
      <th>...</th>
      <th>peak-rpm</th>
      <th>city-mpg</th>
      <th>highway-mpg</th>
      <th>price</th>
      <th>city-L/100km</th>
      <th>highway-L/100km</th>
      <th>fuel-type-diesel</th>
      <th>fuel-type-gas</th>
      <th>aspiration-std</th>
      <th>aspiration-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>symboling</th>
      <td>1.000000</td>
      <td>0.466264</td>
      <td>-0.535987</td>
      <td>-0.365404</td>
      <td>-0.242423</td>
      <td>-0.550160</td>
      <td>-0.233118</td>
      <td>-0.110581</td>
      <td>-0.140019</td>
      <td>-0.008153</td>
      <td>...</td>
      <td>0.279740</td>
      <td>-0.035527</td>
      <td>0.036233</td>
      <td>-0.082391</td>
      <td>0.066171</td>
      <td>-0.029807</td>
      <td>-0.196735</td>
      <td>0.196735</td>
      <td>0.054615</td>
      <td>-0.054615</td>
    </tr>
    <tr>
      <th>normalized-losses</th>
      <td>0.466264</td>
      <td>1.000000</td>
      <td>-0.056661</td>
      <td>0.019424</td>
      <td>0.086802</td>
      <td>-0.373737</td>
      <td>0.099404</td>
      <td>0.112360</td>
      <td>-0.029862</td>
      <td>0.055045</td>
      <td>...</td>
      <td>0.239543</td>
      <td>-0.225016</td>
      <td>-0.181877</td>
      <td>0.133999</td>
      <td>0.238567</td>
      <td>0.181189</td>
      <td>-0.101546</td>
      <td>0.101546</td>
      <td>0.006911</td>
      <td>-0.006911</td>
    </tr>
    <tr>
      <th>wheel-base</th>
      <td>-0.535987</td>
      <td>-0.056661</td>
      <td>1.000000</td>
      <td>0.876024</td>
      <td>0.814507</td>
      <td>0.590742</td>
      <td>0.782097</td>
      <td>0.572027</td>
      <td>0.493244</td>
      <td>0.158018</td>
      <td>...</td>
      <td>-0.360305</td>
      <td>-0.470606</td>
      <td>-0.543304</td>
      <td>0.584642</td>
      <td>0.476153</td>
      <td>0.577576</td>
      <td>0.307237</td>
      <td>-0.307237</td>
      <td>-0.256889</td>
      <td>0.256889</td>
    </tr>
    <tr>
      <th>length</th>
      <td>-0.365404</td>
      <td>0.019424</td>
      <td>0.876024</td>
      <td>1.000000</td>
      <td>0.857170</td>
      <td>0.492063</td>
      <td>0.880665</td>
      <td>0.685025</td>
      <td>0.608971</td>
      <td>0.123952</td>
      <td>...</td>
      <td>-0.285970</td>
      <td>-0.665192</td>
      <td>-0.698142</td>
      <td>0.690628</td>
      <td>0.657373</td>
      <td>0.707108</td>
      <td>0.211187</td>
      <td>-0.211187</td>
      <td>-0.230085</td>
      <td>0.230085</td>
    </tr>
    <tr>
      <th>width</th>
      <td>-0.242423</td>
      <td>0.086802</td>
      <td>0.814507</td>
      <td>0.857170</td>
      <td>1.000000</td>
      <td>0.306002</td>
      <td>0.866201</td>
      <td>0.729436</td>
      <td>0.544885</td>
      <td>0.188822</td>
      <td>...</td>
      <td>-0.245800</td>
      <td>-0.633531</td>
      <td>-0.680635</td>
      <td>0.751265</td>
      <td>0.673363</td>
      <td>0.736728</td>
      <td>0.244356</td>
      <td>-0.244356</td>
      <td>-0.305732</td>
      <td>0.305732</td>
    </tr>
    <tr>
      <th>height</th>
      <td>-0.550160</td>
      <td>-0.373737</td>
      <td>0.590742</td>
      <td>0.492063</td>
      <td>0.306002</td>
      <td>1.000000</td>
      <td>0.307581</td>
      <td>0.074694</td>
      <td>0.180449</td>
      <td>-0.060663</td>
      <td>...</td>
      <td>-0.309974</td>
      <td>-0.049800</td>
      <td>-0.104812</td>
      <td>0.135486</td>
      <td>0.003811</td>
      <td>0.084301</td>
      <td>0.281578</td>
      <td>-0.281578</td>
      <td>-0.090336</td>
      <td>0.090336</td>
    </tr>
    <tr>
      <th>curb-weight</th>
      <td>-0.233118</td>
      <td>0.099404</td>
      <td>0.782097</td>
      <td>0.880665</td>
      <td>0.866201</td>
      <td>0.307581</td>
      <td>1.000000</td>
      <td>0.849072</td>
      <td>0.644060</td>
      <td>0.167438</td>
      <td>...</td>
      <td>-0.279361</td>
      <td>-0.749543</td>
      <td>-0.794889</td>
      <td>0.834415</td>
      <td>0.785353</td>
      <td>0.836921</td>
      <td>0.221046</td>
      <td>-0.221046</td>
      <td>-0.321955</td>
      <td>0.321955</td>
    </tr>
    <tr>
      <th>engine-size</th>
      <td>-0.110581</td>
      <td>0.112360</td>
      <td>0.572027</td>
      <td>0.685025</td>
      <td>0.729436</td>
      <td>0.074694</td>
      <td>0.849072</td>
      <td>1.000000</td>
      <td>0.572609</td>
      <td>0.205928</td>
      <td>...</td>
      <td>-0.256733</td>
      <td>-0.650546</td>
      <td>-0.679571</td>
      <td>0.872335</td>
      <td>0.745059</td>
      <td>0.783465</td>
      <td>0.070779</td>
      <td>-0.070779</td>
      <td>-0.110040</td>
      <td>0.110040</td>
    </tr>
    <tr>
      <th>bore</th>
      <td>-0.140019</td>
      <td>-0.029862</td>
      <td>0.493244</td>
      <td>0.608971</td>
      <td>0.544885</td>
      <td>0.180449</td>
      <td>0.644060</td>
      <td>0.572609</td>
      <td>1.000000</td>
      <td>-0.055390</td>
      <td>...</td>
      <td>-0.267392</td>
      <td>-0.582027</td>
      <td>-0.591309</td>
      <td>0.543155</td>
      <td>0.554610</td>
      <td>0.559112</td>
      <td>0.054458</td>
      <td>-0.054458</td>
      <td>-0.227816</td>
      <td>0.227816</td>
    </tr>
    <tr>
      <th>stroke</th>
      <td>-0.008153</td>
      <td>0.055045</td>
      <td>0.158018</td>
      <td>0.123952</td>
      <td>0.188822</td>
      <td>-0.060663</td>
      <td>0.167438</td>
      <td>0.205928</td>
      <td>-0.055390</td>
      <td>1.000000</td>
      <td>...</td>
      <td>-0.063561</td>
      <td>-0.033956</td>
      <td>-0.034636</td>
      <td>0.082269</td>
      <td>0.036133</td>
      <td>0.047089</td>
      <td>0.241064</td>
      <td>-0.241064</td>
      <td>-0.218233</td>
      <td>0.218233</td>
    </tr>
    <tr>
      <th>compression-ratio</th>
      <td>-0.182196</td>
      <td>-0.114713</td>
      <td>0.250313</td>
      <td>0.159733</td>
      <td>0.189867</td>
      <td>0.259737</td>
      <td>0.156433</td>
      <td>0.028889</td>
      <td>0.001263</td>
      <td>0.187871</td>
      <td>...</td>
      <td>-0.435780</td>
      <td>0.331425</td>
      <td>0.268465</td>
      <td>0.071107</td>
      <td>-0.299372</td>
      <td>-0.223361</td>
      <td>0.985231</td>
      <td>-0.985231</td>
      <td>-0.307522</td>
      <td>0.307522</td>
    </tr>
    <tr>
      <th>horsepower</th>
      <td>0.075810</td>
      <td>0.217300</td>
      <td>0.371178</td>
      <td>0.579795</td>
      <td>0.615056</td>
      <td>-0.087001</td>
      <td>0.757981</td>
      <td>0.822668</td>
      <td>0.566903</td>
      <td>0.098128</td>
      <td>...</td>
      <td>0.107884</td>
      <td>-0.822192</td>
      <td>-0.804579</td>
      <td>0.809607</td>
      <td>0.889482</td>
      <td>0.840627</td>
      <td>-0.169030</td>
      <td>0.169030</td>
      <td>-0.251159</td>
      <td>0.251159</td>
    </tr>
    <tr>
      <th>peak-rpm</th>
      <td>0.279740</td>
      <td>0.239543</td>
      <td>-0.360305</td>
      <td>-0.285970</td>
      <td>-0.245800</td>
      <td>-0.309974</td>
      <td>-0.279361</td>
      <td>-0.256733</td>
      <td>-0.267392</td>
      <td>-0.063561</td>
      <td>...</td>
      <td>1.000000</td>
      <td>-0.115413</td>
      <td>-0.058598</td>
      <td>-0.101616</td>
      <td>0.115830</td>
      <td>0.017694</td>
      <td>-0.475812</td>
      <td>0.475812</td>
      <td>0.190057</td>
      <td>-0.190057</td>
    </tr>
    <tr>
      <th>city-mpg</th>
      <td>-0.035527</td>
      <td>-0.225016</td>
      <td>-0.470606</td>
      <td>-0.665192</td>
      <td>-0.633531</td>
      <td>-0.049800</td>
      <td>-0.749543</td>
      <td>-0.650546</td>
      <td>-0.582027</td>
      <td>-0.033956</td>
      <td>...</td>
      <td>-0.115413</td>
      <td>1.000000</td>
      <td>0.972044</td>
      <td>-0.686571</td>
      <td>-0.949713</td>
      <td>-0.909024</td>
      <td>0.265676</td>
      <td>-0.265676</td>
      <td>0.189237</td>
      <td>-0.189237</td>
    </tr>
    <tr>
      <th>highway-mpg</th>
      <td>0.036233</td>
      <td>-0.181877</td>
      <td>-0.543304</td>
      <td>-0.698142</td>
      <td>-0.680635</td>
      <td>-0.104812</td>
      <td>-0.794889</td>
      <td>-0.679571</td>
      <td>-0.591309</td>
      <td>-0.034636</td>
      <td>...</td>
      <td>-0.058598</td>
      <td>0.972044</td>
      <td>1.000000</td>
      <td>-0.704692</td>
      <td>-0.930028</td>
      <td>-0.951100</td>
      <td>0.198690</td>
      <td>-0.198690</td>
      <td>0.241851</td>
      <td>-0.241851</td>
    </tr>
    <tr>
      <th>price</th>
      <td>-0.082391</td>
      <td>0.133999</td>
      <td>0.584642</td>
      <td>0.690628</td>
      <td>0.751265</td>
      <td>0.135486</td>
      <td>0.834415</td>
      <td>0.872335</td>
      <td>0.543155</td>
      <td>0.082269</td>
      <td>...</td>
      <td>-0.101616</td>
      <td>-0.686571</td>
      <td>-0.704692</td>
      <td>1.000000</td>
      <td>0.789898</td>
      <td>0.801118</td>
      <td>0.110326</td>
      <td>-0.110326</td>
      <td>-0.179578</td>
      <td>0.179578</td>
    </tr>
    <tr>
      <th>city-L/100km</th>
      <td>0.066171</td>
      <td>0.238567</td>
      <td>0.476153</td>
      <td>0.657373</td>
      <td>0.673363</td>
      <td>0.003811</td>
      <td>0.785353</td>
      <td>0.745059</td>
      <td>0.554610</td>
      <td>0.036133</td>
      <td>...</td>
      <td>0.115830</td>
      <td>-0.949713</td>
      <td>-0.930028</td>
      <td>0.789898</td>
      <td>1.000000</td>
      <td>0.958306</td>
      <td>-0.241282</td>
      <td>0.241282</td>
      <td>-0.157578</td>
      <td>0.157578</td>
    </tr>
    <tr>
      <th>highway-L/100km</th>
      <td>-0.029807</td>
      <td>0.181189</td>
      <td>0.577576</td>
      <td>0.707108</td>
      <td>0.736728</td>
      <td>0.084301</td>
      <td>0.836921</td>
      <td>0.783465</td>
      <td>0.559112</td>
      <td>0.047089</td>
      <td>...</td>
      <td>0.017694</td>
      <td>-0.909024</td>
      <td>-0.951100</td>
      <td>0.801118</td>
      <td>0.958306</td>
      <td>1.000000</td>
      <td>-0.158091</td>
      <td>0.158091</td>
      <td>-0.210720</td>
      <td>0.210720</td>
    </tr>
    <tr>
      <th>fuel-type-diesel</th>
      <td>-0.196735</td>
      <td>-0.101546</td>
      <td>0.307237</td>
      <td>0.211187</td>
      <td>0.244356</td>
      <td>0.281578</td>
      <td>0.221046</td>
      <td>0.070779</td>
      <td>0.054458</td>
      <td>0.241064</td>
      <td>...</td>
      <td>-0.475812</td>
      <td>0.265676</td>
      <td>0.198690</td>
      <td>0.110326</td>
      <td>-0.241282</td>
      <td>-0.158091</td>
      <td>1.000000</td>
      <td>-1.000000</td>
      <td>-0.408228</td>
      <td>0.408228</td>
    </tr>
    <tr>
      <th>fuel-type-gas</th>
      <td>0.196735</td>
      <td>0.101546</td>
      <td>-0.307237</td>
      <td>-0.211187</td>
      <td>-0.244356</td>
      <td>-0.281578</td>
      <td>-0.221046</td>
      <td>-0.070779</td>
      <td>-0.054458</td>
      <td>-0.241064</td>
      <td>...</td>
      <td>0.475812</td>
      <td>-0.265676</td>
      <td>-0.198690</td>
      <td>-0.110326</td>
      <td>0.241282</td>
      <td>0.158091</td>
      <td>-1.000000</td>
      <td>1.000000</td>
      <td>0.408228</td>
      <td>-0.408228</td>
    </tr>
    <tr>
      <th>aspiration-std</th>
      <td>0.054615</td>
      <td>0.006911</td>
      <td>-0.256889</td>
      <td>-0.230085</td>
      <td>-0.305732</td>
      <td>-0.090336</td>
      <td>-0.321955</td>
      <td>-0.110040</td>
      <td>-0.227816</td>
      <td>-0.218233</td>
      <td>...</td>
      <td>0.190057</td>
      <td>0.189237</td>
      <td>0.241851</td>
      <td>-0.179578</td>
      <td>-0.157578</td>
      <td>-0.210720</td>
      <td>-0.408228</td>
      <td>0.408228</td>
      <td>1.000000</td>
      <td>-1.000000</td>
    </tr>
    <tr>
      <th>aspiration-turbo</th>
      <td>-0.054615</td>
      <td>-0.006911</td>
      <td>0.256889</td>
      <td>0.230085</td>
      <td>0.305732</td>
      <td>0.090336</td>
      <td>0.321955</td>
      <td>0.110040</td>
      <td>0.227816</td>
      <td>0.218233</td>
      <td>...</td>
      <td>-0.190057</td>
      <td>-0.189237</td>
      <td>-0.241851</td>
      <td>0.179578</td>
      <td>0.157578</td>
      <td>0.210720</td>
      <td>0.408228</td>
      <td>-0.408228</td>
      <td>-1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>22 rows × 22 columns</p>
</div>
<div class="container alert alert-body alert-info">
<pre><code>sometimes we would like to know the significant of the correlation estimate.&lt;br&gt;
&lt;b&gt;P-value: </code></pre>
<p>
<p>What is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.</p>
</p>
<p>By convention, when the</p>
<ul>
<li>
<p>p-value is <span class="math inline">&lt;</span> 0.001: we say there is strong evidence that the correlation is significant.</p>
</li>
<li>
<p>the p-value is <span class="math inline">&lt;</span> 0.05: there is moderate evidence that the correlation is significant.</p>
</li>
<li>
<p>the p-value is <span class="math inline">&lt;</span> 0.1: there is weak evidence that the correlation is significant.</p>
</li>
<li>
<p>the p-value is <span class="math inline">&gt;</span> 0.1: there is no evidence that the correlation is significant.</p>
</li>
</ul>
</div>
<p><strong>We can obtain this information using ``stats`` module in the ``scipy`` library.</strong></p>
<h4 id="wheel-base-vs-price"><code>Wheel-base vs Price</code></h4>
<p>Let’s calculate the Pearson Correlation Coefficient and P-value of <code>wheel-base</code> and <code>price</code>.</p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;wheel-base&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value)  </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.584641822265508 with a P-value of P = 8.076488270733218e-20</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between wheel-base and price is statistically significant, although the linear relationship isn’t extremely strong (~0.585)</strong></p>
<h4 id="horsepower-vs-price"><code>Horsepower vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``horsepower`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;horsepower&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)  </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.8096068016571054 with a P-value of P = 6.273536270650504e-48</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between horsepower and price is statistically significant, and the linear relationship is quite strong (~0.809, close to 1)</strong></p>
<h4 id="length-vs-price"><code>Length vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``length`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;length&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)  </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.6906283804483639 with a P-value of P = 8.016477466159328e-30</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between length and price is statistically significant, and the linear relationship is moderately strong (~0.691).</strong></p>
<h4 id="width-vs-price"><code>Width vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``width`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;width&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value ) </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.7512653440522672 with a P-value of P = 9.20033551048217e-38</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between width and price is statistically significant, and the linear relationship is quite strong (~0.751).</strong></p>
<h4 id="curb-weight-vs-price"><code>Curb-weight vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``curb-weight`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;curb-weight&#39;], df[&#39;price&#39;])
print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)  </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.8344145257702843 with a P-value of P = 2.189577238894065e-53</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between curb-weight and price is statistically significant, and the linear relationship is quite strong (~0.834)</strong></p>
<h4 id="engine-size-vs-price"><code>Engine-size vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``engine-size`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;engine-size&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =&quot;, p_value) </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.8723351674455182 with a P-value of P = 9.265491622200232e-64</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between engine-size and price is statistically significant, and the linear relationship is very strong (~0.872).</strong></p>
<h4 id="bore-vs-price"><code>Bore vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``bore`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;bore&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P =  &quot;, p_value ) </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is 0.5431553832626603 with a P-value of P = 8.049189483935261e-17</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between bore and price is statistically significant, but the linear relationship is only moderate (~0.521).</strong></p>
<h4 id="city-mpg-vs-price"><code>City-mpg vs Price</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``city-mpg`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;city-mpg&#39;], df[&#39;price&#39;])
print(&quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value)  </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is -0.6865710067844678 with a P-value of P = 2.321132065567641e-29</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between city-mpg and price is statistically significant, and the coefficient of ~ -0.687 shows that the relationship is negative and moderately strong.</strong></p>
<h4 id="highway-mpg-vs-price"><code>Highway-mpg vs Price¶</code></h4>
<p><strong>Let’s calculate the Pearson Correlation Coefficient and P-value of ``highway-mpg`` and ``price``.</strong></p>
<pre class="ipython3"><code>pearson_coef, p_value = stats.pearsonr(df[&#39;highway-mpg&#39;], df[&#39;price&#39;])
print( &quot;The Pearson Correlation Coefficient is&quot;, pearson_coef, &quot; with a P-value of P = &quot;, p_value ) </code></pre>
<div class="parsed-literal">
<p>The Pearson Correlation Coefficient is -0.704692265058953 with a P-value of P = 1.7495471144476358e-31</p>
</div>
<p><strong>``Conclusion:``</strong></p>
<p><strong>Since the p-value is &lt; 0.001, the correlation between highway-mpg and price is statistically significant, and the coefficient of ~ -0.705 shows that the relationship is negative and moderately strong.</strong></p>
<h3 id="anova"><code>6. ANOVA</code></h3>
<div class="container alert alert-danger alertdanger">
<p>
<p>The Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:</p>
</p>
<p>
<p>F-test score: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.</p>
</p>
<p>
<p>P-value: P-value tells how statistically significant is our calculated score value.</p>
</p>
<p>
<p>If our price variable is strongly correlated with the variable we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.</p>
</p>
</div>
<p><strong>``Drive Wheels``</strong></p>
<p><strong>Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.</strong></p>
<p><strong>Let’s see if different types ``drive-wheels`` impact ``price``, we group the data.</strong></p>
<pre class="ipython3"><code>grouped_test2=df_gptest[[&#39;drive-wheels&#39;, &#39;price&#39;]].groupby([&#39;drive-wheels&#39;])
grouped_test2.head(2)</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drive-wheels</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>rwd</td>
      <td>13495.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>rwd</td>
      <td>16500.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fwd</td>
      <td>13950.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4wd</td>
      <td>17450.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>fwd</td>
      <td>15250.0</td>
    </tr>
    <tr>
      <th>136</th>
      <td>4wd</td>
      <td>7603.0</td>
    </tr>
  </tbody>
</table>
</div>
<p><strong>We can obtain the values of the method group using the method ``get_group``.</strong></p>
<pre class="ipython3"><code>grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;]</code></pre>
<div class="parsed-literal">
<p>4 17450.0 136 7603.0 140 9233.0 141 11259.0 144 8013.0 145 11694.0 150 7898.0 151 8778.0 Name: price, dtype: float64</p>
</div>
<p><strong>we can use the function ``f_oneway`` in the module ``stats`` to obtain the F-test score and P-value.</strong></p>
<pre class="ipython3"><code># ANOVA
f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;])  

print( &quot;ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val)   </code></pre>
<div class="parsed-literal">
<p>ANOVA results: F= 67.95406500780399 , P = 3.3945443577151245e-23</p>
</div>
<p><strong>This is a great result, with a large F test score showing a strong correlation and a P value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated?</strong></p>
<p><strong>``Separately:``</strong></p>
<pre class="ipython3"><code>f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;])  
print( &quot;fwd and rwd -&gt; ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val )

f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;rwd&#39;)[&#39;price&#39;])     
print( &quot;4wd and rwd -&gt; ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val) 

f_val, p_val = stats.f_oneway(grouped_test2.get_group(&#39;4wd&#39;)[&#39;price&#39;], grouped_test2.get_group(&#39;fwd&#39;)[&#39;price&#39;])  
print(&quot;4wd and fwd -&gt; ANOVA results: F=&quot;, f_val, &quot;, P =&quot;, p_val)   </code></pre>
<div class="parsed-literal">
<p>fwd and rwd -&gt; ANOVA results: F= 130.5533160959111 , P = 2.2355306355677845e-23 4wd and rwd -&gt; ANOVA results: F= 8.580681368924756 , P = 0.004411492211225333 4wd and fwd -&gt; ANOVA results: F= 0.665465750252303 , P = 0.41620116697845666</p>
</div>
<p><strong>Conclusion: ``Important Variables``</strong></p>
<p>
<p>We now have a better idea of what our data looks like and which variables are important to take into account when predicting the car price. We have narrowed it down to the following variables:</p>
</p>
<p>Continuous numerical variables:</p>
<ul>
<li>
<p>Length</p>
</li>
<li>
<p>Width</p>
</li>
<li>
<p>Curb-weight</p>
</li>
<li>
<p>Engine-size</p>
</li>
<li>
<p>Horsepower</p>
</li>
<li>
<p>City-mpg</p>
</li>
<li>
<p>Highway-mpg</p>
</li>
<li>
<p>Wheel-base</p>
</li>
<li>
<p>Bore</p>
</li>
</ul>
<p>Categorical variables:</p>
<ul>
<li>
<p>Drive-wheels</p>
</li>
</ul>
<p>
<p>As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model’s prediction performance.</p>
</p>
<hr />
<h2 id="model-development"><code>7. Model Development</code></h2>
<p><strong>``Objectives``</strong></p>
<p><strong>Develop prediction models</strong></p>
<p><strong>In this section, we will develop several models that will predict the price of the car using the variables or features. This is just an estimate but should give us an objective idea of how much the car should cost.</strong></p>
<p><strong>Some questions we want to ask in this module</strong></p>
<ul>
<li><strong>do I know if the dealer is offering fair value for my trade-in?</strong></li>
<li><strong>do I know if I put a fair value on my car?</strong></li>
<li><strong>Data Analytics, we often use Model Development to help us predict future observations from the data we have.</strong></li>
</ul>
<p><strong>A Model will help us understand the exact relationship between different variables and how these variables are used to predict the result.</strong></p>
<h3 id="linear-regression-and-multiple-linear-regression"><code>1: Linear Regression and Multiple Linear Regression</code></h3>
<h4>
<p>Linear Regression</p>
</h4>
<p>
<p>One example of a Data Model that we will be using is</p>
</p>
<p>Simple Linear Regression.</p>
<p>
<p>Simple Linear Regression is a method to help us understand the relationship between two variables:</p>
</p>
<ul>
<li>
<p>The predictor/independent variable (X)</p>
</li>
<li>
<p>The response/dependent variable (that we want to predict)(Y)</p>
</li>
</ul>
<p>
<p>The result of Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable.</p>
</p>
<p><br /><span class="math display">$$\begin{aligned}
Y: Response \ Variable\\
X: Predictor \ Variables
\end{aligned}$$</span><br /></p>
<p>Linear function:</p>
<p><br /><span class="math display"><em>Y</em><em>h</em><em>a</em><em>t</em> = <em>a</em> + <em>b</em><em>X</em></span><br /></p>
<ul>
<li>
<p>a refers to the intercept of the regression line0, in other words: the value of Y when X is 0</p>
</li>
<li>
<p>b refers to the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit</p>
</li>
</ul>
<p><strong>Create the linear regression object</strong></p>
<pre class="ipython3"><code>lm = LinearRegression()</code></pre>
<p><strong>``How could Highway-mpg help us predict car price?``</strong></p>
<p><strong>we will create a linear function with “highway-mpg” as the predictor variable and the “price” as the response variable.</strong></p>
<pre class="ipython3"><code>X = df[[&#39;highway-mpg&#39;]]
Y = df[&#39;price&#39;]</code></pre>
<p><strong>Fit the linear model using highway-mpg.</strong></p>
<pre class="ipython3"><code>lm.fit(X,Y)</code></pre>
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
<p><strong>We can output a prediction</strong></p>
<pre class="ipython3"><code>Yhat=lm.predict(X)
Yhat[0:5]   </code></pre>
<div class="parsed-literal">
<dl>
<dt>array([16236.50464347, 16236.50464347, 17058.23802179, 13771.3045085 ,</dt>
<dd><p>20345.17153508])</p>
</dd>
</dl>
</div>
<p><strong>What is the value of the intercept (a)?</strong></p>
<pre class="ipython3"><code>lm.intercept_</code></pre>
<div class="parsed-literal">
<p>38423.3058581574</p>
</div>
<p><strong>What is the value of the Slope (b)?</strong></p>
<pre class="ipython3"><code>lm.coef_</code></pre>
<div class="parsed-literal">
<p>array([-821.73337832])</p>
</div>
<p><strong>``What is the final estimated linear model we get?``</strong></p>
<p><strong>As we saw above, we should get a final linear model with the structure:</strong></p>
<p><br /><span class="math display"><em>Y</em><em>h</em><em>a</em><em>t</em> = <em>a</em> + <em>b</em><em>X</em></span><br /></p>
<p><strong>with actual values we get:</strong> <strong>``price = 38423.31 - 821.73 * highway-mpg``</strong></p>
<p><strong>``Train the model using 'engine-size' as the independent variable and 'price' as the dependent variable``</strong></p>
<pre class="ipython3"><code># Extracting independent variable target variables
lm1 = LinearRegression()

# fit in linear model
lm1.fit( df[[&#39;engine-size&#39;]], df[&#39;price&#39;])

print(&quot;What is the value of the intercept (a)? \n {}&quot;.format(lm1.intercept_))
print(&quot;What is the value of the Slope (b)? \n {}&quot;.format(lm1.coef_))
print(&quot;\n Final estimated linear model&quot;)
print(&quot;Yhat=-7963.34 + 166.86*X&quot;)
print(&quot;Price=-7963.34 + 166.86*engine-size&quot;)</code></pre>
<div class="parsed-literal">
<dl>
<dt>What is the value of the intercept (a)?</dt>
<dd><p>-7963.338906281024</p>
</dd>
<dt>What is the value of the Slope (b)?</dt>
<dd><p>[166.86001569]</p>
<p>Final estimated linear model</p>
</dd>
</dl>
<p>Yhat=-7963.34 + 166.86*X Price=-7963.34 + 166.86*engine-size</p>
</div>
<p><strong>``Multiple Linear Regression``</strong></p>
<p>
<p>What if we want to predict car price using more than one variable?</p>
</p>
<p>
<p>If we want to use more variables in our model to predict car price, we can use Multiple Linear Regression. Multiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response (dependent) variable and two or more predictor (independent) variables. Most of the real-world regression models involve multiple predictors. We will illustrate the structure by using four predictor variables, but these results can generalize to any integer:</p>
</p>
<p><br /><span class="math display">$$\begin{aligned}
Y: Response \ Variable\\
X_1 :Predictor \ Variable \ 1\\
X_2: Predictor\ Variable \ 2\\
X_3: Predictor\ Variable \ 3\\
X_4: Predictor\ Variable \ 4\\
\end{aligned}$$</span><br /></p>
<p><br /><span class="math display">$$\begin{aligned}
a: intercept\\
b_1 :coefficients \ of\ Variable \ 1\\
b_2: coefficients \ of\ Variable \ 2\\
b_3: coefficients \ of\ Variable \ 3\\
b_4: coefficients \ of\ Variable \ 4\\
\end{aligned}$$</span><br /></p>
<p><strong>The equation is given by</strong></p>
<p><br /><span class="math display"><em>Y</em><em>h</em><em>a</em><em>t</em> = <em>a</em> + <em>b</em><sub>1</sub><em>X</em><sub>1</sub> + <em>b</em><sub>2</sub><em>X</em><sub>2</sub> + <em>b</em><sub>3</sub><em>X</em><sub>3</sub> + <em>b</em><sub>4</sub><em>X</em><sub>4</sub></span><br /></p>
<p><strong>From the previous section we know that other good predictors of price could be:</strong></p>
<ul>
<li><code>Horsepower</code></li>
<li><code>Curb-weight</code></li>
<li><code>Engine-size</code></li>
<li><code>Highway-mpg</code></li>
</ul>
<p><strong>Let’s develop a model using these variables as the predictor variables</strong></p>
<pre class="ipython3"><code>lm3 = LinearRegression() # creating regression variable
Z = df[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]] # extracting multiple independent variables

#Fit the linear model using the four above-mentioned variables.
lm3.fit(Z, df[&#39;price&#39;])
print(&quot;What is the value of the intercept (a)? \n {}&quot;.format(lm3.intercept_))
print(&quot;What are the values of the coefficients (b1, b2, b3, b4)? \n {}&quot;.format(lm3.coef_))
print(&quot;\n Final estimated linear model&quot;)
print(f&quot;\n Price = {lm3.intercept_} + {lm3.coef_[0]}*horsepower + {lm3.coef_[1]}*curb-weight + {lm3.coef_[2]}*engine-size + {lm3.coef_[3]}*highway-mpg&quot;)</code></pre>
<div class="parsed-literal">
<dl>
<dt>What is the value of the intercept (a)?</dt>
<dd><p>-15811.863767729217</p>
</dd>
<dt>What are the values of the coefficients (b1, b2, b3, b4)?</dt>
<dd><p>[53.53022809 4.70805253 81.51280006 36.1593925 ]</p>
<p>Final estimated linear model</p>
<p>Price = -15811.863767729217 + 53.5302280860699*horsepower + 4.708052531299506*curb-weight + 81.51280005759963*engine-size + 36.15939250212034*highway-mpg</p>
</dd>
</dl>
</div>
<p><strong>Create and train a Multiple Linear Regression model ``lm4`` where the response variable is price, and the predictor variable is ``normalized-losses`` and ``highway-mpg``.</strong></p>
<pre class="ipython3"><code>lm4 = LinearRegression()
lm4.fit(df[[&#39;normalized-losses&#39;,&#39;highway-mpg&#39;]],df[&#39;price&#39;]) 
print(&quot;What is the value of the intercept (a)? \n {}&quot;.format(lm4.intercept_))
print(&quot;What are the values of the coefficients (b1, b2, b3, b4)? \n {}&quot;.format(lm4.coef_))
print(&quot;\n Estimated linear model&quot;)
print(f&quot;\n Price = {lm4.intercept_} + {lm4.coef_[0]}*normalized-losses  {lm4.coef_[1]}*highway-mpg &quot;)</code></pre>
<div class="parsed-literal">
<dl>
<dt>What is the value of the intercept (a)?</dt>
<dd><p>38201.31327245727</p>
</dd>
<dt>What are the values of the coefficients (b1, b2, b3, b4)?</dt>
<dd><p>[ 1.49789586 -820.45434016]</p>
<p>Estimated linear model</p>
<p>Price = 38201.31327245727 + 1.4978958634133372*normalized-losses -820.4543401631862*highway-mpg</p>
</dd>
</dl>
</div>
<h3 id="model-evaluation-using-visualization"><code>2: Model Evaluation using Visualization</code></h3>
<p><strong>Now that we’ve developed some models, how do we evaluate our models and how do we choose the best one? One way to do this is by using visualization.</strong></p>
<h3 id="regression-plot"><code>Regression Plot</code></h3>
<p><strong>When it comes to simple linear regression, an excellent way to visualize the fit of our model is by using regression plots.</strong></p>
<p><strong>This plot will show a combination of a scattered data points (a scatter plot), as well as the fitted linear regression line going through the data. This will give us a reasonable estimate of the relationship between the two variables, the strength of the correlation, as well as the direction (positive or negative correlation).</strong></p>
<p><strong>Let’s visualize highway-mpg as potential predictor variable of price:</strong></p>
<pre class="ipython3"><code>sns.regplot(x = &#39;highway-mpg&#39;, y = &#39;price&#39;,data = df)
plt.ylim(0,)</code></pre>
<div class="parsed-literal">
<p>(0.0, 48170.63000618988)</p>
</div>
<p><img src="output_196_1.png" alt="image" /></p>
<p><strong>We can see from this plot that price is negatively correlated to highway-mpg, since the regression slope is negative. One thing to keep in mind when looking at a regression plot is to pay attention to how scattered the data points are around the regression line. This will give you a good indication of the variance of the data, and whether a linear model would be the best fit or not. If the data is too far off from the line, this linear model might not be the best model for this data.</strong></p>
<h3 id="residual-plot"><code>Residual Plot</code></h3>
<p>A good way to visualize the variance of the data is to use a residual plot.</p>
<p>What is a residual?</p>
<p>The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.</p>
<p>So what is a residual plot?</p>
<p>A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.</p>
<p>What do we pay attention to when looking at a residual plot?</p>
<p>We look at the spread of the residuals:</p>
<ul>
<li>If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data. Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.</li>
</ul>
<pre class="ipython3"><code>width = 12
height = 10
plt.figure(figsize=(width, height))
sns.residplot(df[&#39;highway-mpg&#39;], df[&#39;price&#39;])
plt.show()</code></pre>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborn_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be <span class="title-ref">data</span>, and passing other arguments without an explicit keyword will result in an error or misinterpretation.</dt>
<dd><p>warnings.warn(</p>
</dd>
</dl>
</div>
<p><img src="output_198_1.png" alt="image" /></p>
<p>What is this plot telling us?</p>
<p>We can see from this residual plot that the residuals are not randomly spread around the x-axis, which leads us to believe that maybe a non-linear model is more appropriate for this data.</p>
<h3 id="multiple-linear-regression"><code>Multiple Linear Regression</code></h3>
<p>How do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because you can’t visualize it with regression or residual plot.</p>
<p>One way to look at the fit of the model is by looking at the distribution plot: We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.</p>
<p>First lets make a prediction</p>
<pre class="ipython3"><code>Y_hat = lm.predict(df[[&#39;highway-mpg&#39;]])
print(&#39;Simple Linear Regressionn&#39;)

# plot
plt.figure(figsize=(width, height))


ax1 = sns.distplot(df[&#39;price&#39;], hist=False, color=&quot;r&quot;)
sns.distplot(Y_hat, hist=False, color=&quot;b&quot;, ax=ax1)

plt.legend([&quot;Actual Value&quot;,&quot;Fitted Values&quot;])
plt.title(&#39;Actual vs Fitted Values for Price&#39;)
plt.xlabel(&#39;Price (in dollars)&#39;)
plt.ylabel(&#39;Proportion of Cars&#39;)

plt.show()
plt.close()</code></pre>
<div class="parsed-literal">
<p>Simple Linear Regressionn</p>
</div>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_200_2.png" alt="image" /></p>
<pre class="ipython3"><code>Y_hat = lm3.predict(df[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]])
print(&quot;Multiple Linear Regression&quot;)
# plot
plt.figure(figsize=(width, height))


ax1 = sns.distplot(df[&#39;price&#39;], hist=False, color=&quot;r&quot;)
sns.distplot(Y_hat, hist=False, color=&quot;b&quot;, ax=ax1)

plt.legend([&quot;Actual Value&quot;,&quot;Fitted Values&quot;])
plt.title(&#39;Actual vs Fitted Values for Price&#39;)
plt.xlabel(&#39;Price (in dollars)&#39;)
plt.ylabel(&#39;Proportion of Cars&#39;)

plt.show()
plt.close()</code></pre>
<div class="parsed-literal">
<p>Multiple Linear Regression</p>
</div>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_201_2.png" alt="image" /></p>
<p><strong>We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.</strong></p>
<h3 id="polynomial-regression-and-pipeline"><code>3: Polynomial Regression and Pipeline</code></h3>
<p>Polynomial regression is a particular case of the general linear regression model or multiple linear regression models.</p>
<p>We get non-linear relationships by squaring or setting higher-order terms of the predictor variables.</p>
<p>There are different orders of polynomial regression:</p>
<center>
<p>Quadratic - 2nd order</p>
</center>
<p><br /><span class="math display"><em>Y</em><em>h</em><em>a</em><em>t</em> = <em>a</em> + <em>b</em><sub>1</sub><em>X</em> + <em>b</em><sub>2</sub><em>X</em><sup>2</sup></span><br /></p>
<center>
<p>Cubic - 3rd order</p>
</center>
<p><br /><span class="math display">$$\begin{aligned}
Yhat = a + b_1 X +b_2 X^2 +b_3 X^3\\\\
\end{aligned}$$</span><br /></p>
<center>
<p>Higher order:</p>
</center>
<p><br /><span class="math display">$$\begin{aligned}
Y = a + b_1 X +b_2 X^2 +b_3 X^3 ....\\\\
\end{aligned}$$</span><br /></p>
<p><strong>We saw earlier that a linear model did not provide the best fit while using highway-mpg as the predictor variable. Let’s see if we can try fitting a polynomial model to the data instead.</strong></p>
<p><strong>We will use the following function to plot the data:</strong></p>
<pre class="ipython3"><code>def Polly_Plot(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(15, 55, 100)
    y_new = model(x_new)

    plt.plot(independent_variable, dependent_variabble, &#39;.&#39;, x_new, y_new, &#39;-&#39;)
    plt.title(&#39;Polynomial Fit with Matplotlib for Price ~ Length&#39;)
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel(&#39;Price of Cars&#39;)

    plt.show()
    plt.close()</code></pre>
<pre class="ipython3"><code># Lets get the variables
x = df[&#39;highway-mpg&#39;]
y = df[&#39;price&#39;]

# Here we use a polynomial of the 3rd order (cubic) 
f = np.polyfit(x, y, 3)
p = np.poly1d(f)
print(p)</code></pre>
<div class="parsed-literal">
<p>3 2 -1.557 x + 204.8 x - 8965 x + 1.379e+05</p>
</div>
<p><strong>Let’s plot the function</strong></p>
<pre class="ipython3"><code>Polly_Plot(p, x, y, &#39;highway-mpg&#39;)</code></pre>
<p><img src="output_206_0.png" alt="image" /></p>
<p><strong>We can already see from plotting that this polynomial model performs better than the linear model. This is because the generated polynomial function ``hits`` more of the data points.</strong></p>
<p><strong>``Create 11 order polynomial model`` with the variables x and y from above</strong></p>
<pre class="ipython3"><code>f1 = np.polyfit(x, y, 11)
p1 = np.poly1d(f1)
print(p1)
Polly_Plot(p1,x,y, &#39;Highway MPG&#39;)</code></pre>
<div class="parsed-literal">
<p>11 10 9 8 7 -1.243e-08 x + 4.722e-06 x - 0.0008028 x + 0.08056 x - 5.297 x 6 5 4 3 2 + 239.5 x - 7588 x + 1.684e+05 x - 2.565e+06 x + 2.551e+07 x - 1.491e+08 x + 3.879e+08</p>
</div>
<p><img src="output_208_1.png" alt="image" /></p>
<p><strong>The analytical expression for Multivariate Polynomial function gets complicated. For example, the expression for a second-order (degree=2)polynomial with two variables is given by:</strong></p>
<p><br /><span class="math display"><em>Y</em><em>h</em><em>a</em><em>t</em> = <em>a</em> + <em>b</em><sub>1</sub><em>X</em><sub>1</sub> + <em>b</em><sub>2</sub><em>X</em><sub>2</sub> + <em>b</em><sub>3</sub><em>X</em><sub>1</sub><em>X</em><sub>2</sub> + <em>b</em><sub>4</sub><em>X</em><sub>1</sub><sup>2</sup> + <em>b</em><sub>5</sub><em>X</em><sub>2</sub><sup>2</sup></span><br /></p>
<p><strong>We can perform a polynomial transform on multiple features. First, we import the module:</strong></p>
<pre class="ipython3"><code>from sklearn.preprocessing import PolynomialFeatures</code></pre>
<p><strong>We create a PolynomialFeatures object of degree 2:</strong></p>
<pre class="ipython3"><code>pr=PolynomialFeatures(degree=2)
pr</code></pre>
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PolynomialFeatures()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures()</pre></div></div></div></div></div>
<pre class="ipython3"><code>Z_pr=pr.fit_transform(Z)</code></pre>
<p><strong>The original data is of 201 samples and 4 features</strong></p>
<pre class="ipython3"><code>Z.shape</code></pre>
<div class="parsed-literal">
<p>(201, 4)</p>
</div>
<p><strong>after the transformation, there 201 samples and 15 features</strong></p>
<pre class="ipython3"><code>Z_pr.shape</code></pre>
<div class="parsed-literal">
<p>(201, 15)</p>
</div>
<h3 id="pipeline"><code>Pipeline</code></h3>
<p><strong>Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use StandardScaler - to Normalize the data as a step in our pipeline .</strong></p>
<pre class="ipython3"><code>from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler</code></pre>
<p><strong>We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.</strong></p>
<pre class="ipython3"><code>Input=[(&#39;scale&#39;,StandardScaler()), (&#39;polynomial&#39;, PolynomialFeatures(include_bias=False)), (&#39;model&#39;,LinearRegression())]</code></pre>
<p><strong>we input the list as an argument to the pipeline constructor</strong></p>
<pre class="ipython3"><code>pipe=Pipeline(Input)
pipe</code></pre>
<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler()),
                (&#x27;polynomial&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;model&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler()),
                (&#x27;polynomial&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;model&#x27;, LinearRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures(include_bias=False)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>
<p><strong>We can normalize the data, perform a transform and fit the model simultaneously.</strong></p>
<pre class="ipython3"><code>pipe.fit(Z,y)</code></pre>
<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler()),
                (&#x27;polynomial&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;model&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler()),
                (&#x27;polynomial&#x27;, PolynomialFeatures(include_bias=False)),
                (&#x27;model&#x27;, LinearRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures(include_bias=False)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>
<p><strong>Similarly, we can normalize the data, perform a transform and produce a prediction simultaneously</strong></p>
<pre class="ipython3"><code>ypipe=pipe.predict(Z)
ypipe[0:4]</code></pre>
<div class="parsed-literal">
<p>array([13102.93329646, 13102.93329646, 18226.43450275, 10391.09183955])</p>
</div>
<h3 id="measures-for-in-sample-evaluation"><code>4: Measures for In-Sample Evaluation</code></h3>
<p>When evaluating our models, not only do we want to visualize the results, but we also want a quantitative measure to determine how accurate the model is.</p>
<p>Two very important measures that are often used in Statistics to determine the accuracy of a model are:</p>
<ul>
<li>R^2 / R-squared</li>
<li>Mean Squared Error (MSE)</li>
</ul>
<p>R-squared R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.</p>
<p>The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.</p>
<p>Mean Squared Error (MSE)</p>
<p>The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (ŷ).</p>
<p><strong>Let’s calculate the R^2</strong></p>
<pre class="ipython3"><code>#highway_mpg_fit
X = df[[&#39;highway-mpg&#39;]]
Y = df[&#39;price&#39;]
lm.fit(X, Y)
# Find the R^2
print(&#39;The R-square is: &#39;, lm.score(X, Y))</code></pre>
<div class="parsed-literal">
<p>The R-square is: 0.4965911884339176</p>
</div>
<p><strong>We can say that ~ 49.659% of the variation of the price is explained by this simple linear model ``horsepower_fit``.</strong></p>
<p><strong>To calculate the MSE</strong></p>
<p><strong>lets import the function mean_squared_error from the module metrics</strong></p>
<pre class="ipython3"><code>from sklearn.metrics import mean_squared_error</code></pre>
<p><strong>we compare the predicted results with the actual results</strong></p>
<pre class="ipython3"><code>Yhat=lm.predict(X)
mse = mean_squared_error(df[&#39;price&#39;], Yhat)
print(&#39;The mean square error of price and predicted value is: &#39;, mse)</code></pre>
<div class="parsed-literal">
<p>The mean square error of price and predicted value is: 31635042.944639888</p>
</div>
<h4 id="model-2-multiple-linear-regression">Model <code>2: Multiple Linear Regression</code></h4>
<p><strong>Let’s calculate the R^2</strong></p>
<pre class="ipython3"><code># fit the model 
lm.fit(Z, df[&#39;price&#39;])
# Find the R^2
print(&#39;The R-square is: &#39;, lm.score(Z, df[&#39;price&#39;]))</code></pre>
<div class="parsed-literal">
<p>The R-square is: 0.8093732522175299</p>
</div>
<p>We can say that ~ 80.896 % of the variation of price is explained by this multiple linear regression “multi_fit”.</p>
<p>Let’s calculate the MSE</p>
<p>we produce a prediction</p>
<pre class="ipython3"><code>Y_predict_multifit = lm.predict(Z)</code></pre>
<p><strong>we compare the predicted results with the actual results</strong></p>
<pre class="ipython3"><code>print(&#39;The mean square error of price and predicted value using multifit is: &#39;, \
      mean_squared_error(df[&#39;price&#39;], Y_predict_multifit))</code></pre>
<div class="parsed-literal">
<p>The mean square error of price and predicted value using multifit is: 11979300.349818882</p>
</div>
<p><strong>``Model 3: Polynomial Fit``</strong></p>
<p><strong>To calculate the R^2, let’s import the function r2_score from the module metrics as we are using a different function</strong></p>
<pre class="ipython3"><code>from sklearn.metrics import r2_score</code></pre>
<p><strong>We apply the function to get the value of r^2</strong></p>
<pre class="ipython3"><code>r_squared = r2_score(y, p(x)) # degree 3 polynomial
print(&#39;The R-square value is: &#39;, r_squared)</code></pre>
<div class="parsed-literal">
<p>The R-square value is: 0.674194666390652</p>
</div>
<p><strong>We can say that ~ 67.419 % of the variation of price is explained by this polynomial fit</strong></p>
<p><strong>We can also calculate the MSE:</strong></p>
<pre class="ipython3"><code>mean_squared_error(df[&#39;price&#39;], p(x)) # degree 3 polynomial</code></pre>
<div class="parsed-literal">
<p>20474146.426361218</p>
</div>
<pre class="ipython3"><code>r_squared = r2_score(y, p1(x)) #  degree 11 polynomial
print(&#39;The R-square value is: &#39;, r_squared)</code></pre>
<div class="parsed-literal">
<p>The R-square value is: 0.702376909204032</p>
</div>
<pre class="ipython3"><code>mean_squared_error(df[&#39;price&#39;], p1(x)) # degree 11 polynomial</code></pre>
<div class="parsed-literal">
<p>18703127.64164033</p>
</div>
<h3 id="prediction-and-decision-making"><code>5: Prediction and Decision Making</code></h3>
<h4 id="prediction"><code>Prediction</code></h4>
<p><strong>In the previous section, we trained the model using the method fit. Now we will use the method predict to produce a prediction. we will use pyplot for plotting, also be using some functions from numpy.</strong></p>
<p><strong>Create a new input</strong></p>
<pre class="ipython3"><code>new_input=np.arange(1, 101, 1).reshape(-1, 1) # 100 sample inputs</code></pre>
<p><strong>Fit the model</strong></p>
<pre class="ipython3"><code>lm.fit(X, Y)
lm</code></pre>
<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" checked><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
<p><strong>Produce a prediction</strong></p>
<pre class="ipython3"><code>yhat=lm.predict(new_input)
yhat[0:5]</code></pre>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagessklearnbase.py:450: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names</dt>
<dd><p>warnings.warn(</p>
</dd>
</dl>
</div>
<div class="parsed-literal">
<dl>
<dt>array([37601.57247984, 36779.83910151, 35958.10572319, 35136.37234487,</dt>
<dd><p>34314.63896655])</p>
</dd>
</dl>
</div>
<p><strong>we can plot the data</strong></p>
<pre class="ipython3"><code>plt.plot(new_input, yhat)
plt.show()</code></pre>
<p><img src="output_258_0.png" alt="image" /></p>
<h3 id="decision-making-determining-a-good-model-fit"><code>Decision Making: Determining a Good Model Fit</code></h3>
<p>Now that we have visualized the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?</p>
<p>What is a good R-squared value? When comparing models, the model with the higher R-squared value is a better fit for the data.</p>
<p>What is a good MSE? When comparing models, the model with the smallest MSE value is a better fit for the data.</p>
<p><strong>``Let's take a look at the values for the different models.``</strong></p>
<p><strong>Simple Linear Regression: Using Highway-mpg as a Predictor Variable of Price.</strong> - R-squared: 0.49659118843391759 - MSE: 3.16 x10^7</p>
<p><strong>Multiple Linear Regression: Using Horsepower, Curb-weight, Engine-size, and Highway-mpg as Predictor Variables of Price.</strong> -R-squared: 0.80896354913783497 - MSE: 1.2 x10^7</p>
<p><strong>Polynomial Fit: Using Highway-mpg as a Predictor Variable of Price.</strong> - R-squared: 0.6741946663906514 - MSE: 2.05 x 10^7</p>
<p><strong>``Simple Linear Regression model (SLR) vs Multiple Linear Regression model (MLR)``</strong> <strong>Usually, the more variables you have, the better your model is at predicting, but this is not always true. Sometimes you may not have enough data, you may run into numerical problems, or many of the variables may not be useful and or even act as noise. As a result, you should always check the MSE and R^2.</strong></p>
<p><strong>So to be able to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.</strong></p>
<ul>
<li>MSE: The MSE of SLR is 3.16x10^7 while MLR has an MSE of 1.2 x10^7. The MSE of MLR is much smaller.</li>
<li>R-squared: In this case, we can also see that there is a big difference between the R-squared of the SLR and the R-squared of the MLR. The R-squared for the SLR (0.497) is very small compared to the R-squared for the MLR (0.809).</li>
</ul>
<p><strong>This R-squared in combination with the MSE show that MLR seems like the better model fit in this case, compared to SLR.</strong></p>
<p><strong>``Simple Linear Model (SLR) vs Polynomial Fit``</strong></p>
<ul>
<li>MSE: We can see that Polynomial Fit brought down the MSE, since this MSE is smaller than the one from the SLR.</li>
<li>R-squared: The R-squared for the Polyfit is larger than the R-squared for the SLR, so the Polynomial Fit also brought up the R-squared quite a bit.</li>
</ul>
<p><strong>Since the Polynomial Fit resulted in a lower MSE and a higher R-squared, we can conclude that this was a better fit model than the simple linear regression for predicting Price with Highway-mpg as a predictor variable.</strong></p>
<p><strong>``Multiple Linear Regression (MLR) vs Polynomial Fit``</strong> - MSE: The MSE for the MLR is smaller than the MSE for the Polynomial Fit. -R-squared: The R-squared for the MLR is also much larger than for the Polynomial Fit.</p>
<h3 id="r-squared-in-combination-with-the-mse-show-that-mlr-seems-like-the-better-model-fit-in-this-case-compared-to-slr-and-polynomial-fit.">R-Squared in combination with the MSE show that MLR seems like the better model fit in this case, compared to SLR and Polynomial Fit.</h3>
<h2 id="model-evaluation-and-refinement"><code>Model Evaluation and Refinement</code></h2>
<p>In the previous section, we have evalulated the model using in-sample evaluation. In-sample evaluation tells us how well our model fits the data already given to train it. It does not give us an estimate of how well the train model can predict new data. The solution is to split our data up, use the in-sample data or training data to train the model. The rest of the data, called Test Data, is used as out-of-sample data. This data is then used to approximate, how the model performs in the real world. Separating data into training and testing sets is an important part of model evaluation. We use the test data to get an idea how our model will perform in the real world.</p>
<p><strong>``Functions for Plotting``</strong></p>
<pre class="ipython3"><code>def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title): # red function : actual, Blue Function : predicted
    width = 12
    height = 10
    plt.figure(figsize=(width, height))

    ax1 = sns.distplot(RedFunction, hist=False, color=&quot;r&quot;, label=RedName)
    ax2 = sns.distplot(BlueFunction, hist=False, color=&quot;b&quot;, label=BlueName, ax=ax1)

    plt.title(Title)
    plt.xlabel(&#39;Price (in dollars)&#39;)
    plt.ylabel(&#39;Proportion of Cars&#39;)
    plt.legend([RedName,BlueName])

    plt.show()
    plt.close()</code></pre>
<pre class="ipython3"><code>def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))


    #training data 
    #testing data 
    # lr:  linear regression object 
    #poly_transform:  polynomial transformation object 

    xmax=max([xtrain.values.max(), xtest.values.max()])

    xmin=min([xtrain.values.min(), xtest.values.min()])

    x=np.arange(xmin, xmax, 0.1)


    plt.plot(xtrain, y_train, &#39;or&#39;, label=&#39;Training Data&#39;)
    plt.plot(xtest, y_test, &#39;og&#39;, label=&#39;Test Data&#39;)
    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label=&#39;Predicted Function&#39;)
    plt.ylim([-10000, 60000])
    plt.ylabel(&#39;Price&#39;)
    plt.legend()</code></pre>
<h3 id="part-1-training-and-testing"><code>Part 1: Training and Testing</code></h3>
<p><strong>An important step in testing your model is to split your data into training and testing data.</strong></p>
<pre class="ipython3"><code># We will place the target data price in a separate dataframe y:
y_data = df[&#39;price&#39;] 

#drop price data in x data
x_data = df.drop(&#39;price&#39;,axis=1)</code></pre>
<p><strong>Now we randomly split our data into training and testing data using the function train_test_split.</strong></p>
<pre class="ipython3"><code>from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)


print(&quot;number of test samples :&quot;, x_test.shape[0])
print(&quot;number of training samples:&quot;,x_train.shape[0])</code></pre>
<div class="parsed-literal">
<p>number of test samples : 21 number of training samples: 180</p>
</div>
<p><strong>The test_size parameter sets the proportion of data that is split into the testing set. In the above, the testing set is set to 10% of the total dataset.</strong></p>
<p><strong>Let’s Calculate the R^2 on the test data (x_train, x_test, y_train, y_test):</strong></p>
<pre class="ipython3"><code># let&#39;s import LinearRegression from the module linear_model.
from sklearn.linear_model import LinearRegression

#We create a Linear Regression object:
lre=LinearRegression()

#we fit the model using the feature horsepower
lre.fit(x_train[[&#39;horsepower&#39;]], y_train)

# Let&#39;s Calculate the R^2 on the test data:
test =  lre.score(x_test[[&#39;horsepower&#39;]], y_test)
print(&#39;the R^2 on the Test data:&#39;, test)

# Let&#39;s Calculate the R^2 on the Train data:
train = lre.score(x_train[[&#39;horsepower&#39;]], y_train)
print(&#39;the R^2 on the Train data:&#39;, train)</code></pre>
<div class="parsed-literal">
<p>the R^2 on the Test data: 0.3635480624962413 the R^2 on the Train data: 0.662028747521533</p>
</div>
<p><strong>we can see the R^2 is much smaller using the test data. This is because almost 90% of the data is used to train the model becuase of which the model is able of explain 66% of the seen data(i.e. 90% of the total data) and only 10% of the data to test it because of which it can explain only 36% of that unseen 10% data. In short, the model has good accuracy but very low precision.</strong></p>
<p><strong>Now, let’s split up the data set such that 40% of the data samples will be utilized for testing, set the parameter “random_state” equal to zero. The output of the function should be the following: “x_train_1” , “x_test_1”, “y_train_1” and “y_test_1”</strong></p>
<pre class="ipython3"><code>x_train1,x_test1, y_train1,  y_test1 = train_test_split(x_data, y_data,test_size=0.4, random_state=0 )
print(&quot;number of test samples :&quot;, x_test1.shape[0])
print(&quot;number of training samples:&quot;,x_train1.shape[0])

# training model and calculating R^2
lre1=LinearRegression()
lre1.fit(x_train1[[&#39;horsepower&#39;]], y_train1)
# Let&#39;s Calculate the R^2 on the test data:
test1 =  lre1.score(x_test1[[&#39;horsepower&#39;]], y_test1)
print(&#39;the R^2 on the Test data:&#39;, test1)

# Let&#39;s Calculate the R^2 on the Train data:
train1 = lre1.score(x_train1[[&#39;horsepower&#39;]], y_train1)
print(&#39;the R^2 on the Train data:&#39;, train1)</code></pre>
<div class="parsed-literal">
<p>number of test samples : 81 number of training samples: 120 the R^2 on the Test data: 0.7139737368233016 the R^2 on the Train data: 0.5754853866574969</p>
</div>
<p><strong>We can see a big jump precision to 71% from 36% and slight drop in accuracy from 66% to 57%. But this model can predict real world data more precisely.</strong></p>
<h3 id="cross-validation-score"><code>Cross-validation Score</code></h3>
<p><strong>Sometimes you do not have sufficient testing data; as a result, you may want to perform Cross-validation. Let’s go over several methods that you can use for Cross-validation.</strong></p>
<pre class="ipython3"><code># Lets import model_selection from the module cross_val_score.
from sklearn.model_selection import cross_val_score

# We input the object, the feature in this case &#39; horsepower&#39;, the target data (y_data).
# The parameter &#39;cv&#39; determines the number of folds; in this case 4.
Rcross = cross_val_score(lre, x_data[[&#39;horsepower&#39;]], y_data, cv=4)

# The default scoring is R^2; each element in the array has the average R^2 value in the fold:
Rcross</code></pre>
<div class="parsed-literal">
<p>array([0.77465419, 0.51718424, 0.74814454, 0.04825398])</p>
</div>
<p><strong>We can calculate the average and standard deviation of our estimate:</strong></p>
<pre class="ipython3"><code>print(&quot;The mean of the folds are&quot;, Rcross.mean(), &quot;and the standard deviation is&quot; , Rcross.std())</code></pre>
<div class="parsed-literal">
<p>The mean of the folds are 0.5220592359225417 and the standard deviation is 0.291304806661184</p>
</div>
<p><strong>You can also use the function ``cross_val_predict`` to predict the output. The function splits up the data into the specified number of folds, using one fold for testing and the other folds are used for training. First import the function:</strong></p>
<pre class="ipython3"><code>from sklearn.model_selection import cross_val_predict</code></pre>
<p><strong>We input the object, the feature in this case ``horsepower`` , the target data y_data. The parameter ``cv`` determines the number of folds; in this case 4. We can produce an output that was obtained for each element when it was in the test set.:</strong></p>
<pre class="ipython3"><code>yhat = cross_val_predict(lre,x_data[[&#39;horsepower&#39;]], y_data,cv=4)
yhat[0:5] # yhat has total 201 values</code></pre>
<div class="parsed-literal">
<dl>
<dt>array([14142.23793549, 14142.23793549, 20815.3029844 , 12745.549902 ,</dt>
<dd><p>14762.9881726 ])</p>
</dd>
</dl>
</div>
<h3 id="part-2-overfitting-underfitting-and-model-selection"><code>Part 2: Overfitting, Underfitting and Model Selection</code></h3>
<p><strong>It turns out that the test data sometimes referred to as the out of sample data is a much better measure of how well your model performs in the real world. One reason for this is overfitting; let’s go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.</strong></p>
<p><strong>Let’s create Multiple linear regression objects and train the model using ‘horsepower’, ‘curb-weight’, ‘engine-size’ and ‘highway-mpg’ as features.</strong></p>
<pre class="ipython3"><code>lr = LinearRegression()
lr.fit(x_train[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]], y_train) # 10% test data</code></pre>
<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" checked><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
<p><strong>Prediction using training data:</strong></p>
<pre class="ipython3"><code>yhat_train = lr.predict(x_train[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]]) 
yhat_train[0:5]</code></pre>
<div class="parsed-literal">
<dl>
<dt>array([ 7426.34910902, 28324.42490838, 14212.74872339, 4052.80810192,</dt>
<dd><p>34499.8541269 ])</p>
</dd>
</dl>
</div>
<p><strong>Prediction using test data:</strong></p>
<pre class="ipython3"><code>yhat_test = lr.predict(x_test[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]]) 
yhat_test[0:5]</code></pre>
<div class="parsed-literal">
<dl>
<dt>array([11349.68099115, 5884.25292475, 11208.31007475, 6641.03017109,</dt>
<dd><p>15565.98722248])</p>
</dd>
</dl>
</div>
<p><strong>Let’s perform some model evaluation using our training and testing data separately.</strong></p>
<pre class="ipython3"><code>Title = &#39;Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution&#39; 
DistributionPlot(y_train, yhat_train, &quot;Actual Values (Train)&quot;, &quot;Predicted Values (Train)&quot;, Title) </code></pre>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_290_1.png" alt="image" /></p>
<p><strong>Figure 1: Plot of predicted values using the training data compared to the training data.</strong></p>
<p><strong>So far the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values.</strong></p>
<pre class="ipython3"><code>Title=&#39;Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data&#39;
DistributionPlot(y_test,yhat_test,&quot;Actual Values (Test)&quot;,&quot;Predicted Values (Test)&quot;,Title)</code></pre>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_292_1.png" alt="image" /></p>
<p><strong>Figur 2: Plot of predicted value using the test data compared to the test data.</strong></p>
<p><strong>Comparing Figure 1 and Figure 2; it is evident the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent where the ranges are from 5000 to 15 000. This is where the distribution shape is exceptionally different. Let’s see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.</strong></p>
<h3 id="overfitting"><code>Overfitting</code></h3>
<p><strong>Overfitting occurs when the model fits the noise, not the underlying process. Therefore when testing your model using the test-set, your model does not perform well as it is modelling noise, not the underlying process that generated the relationship. Let’s create a degree 5 polynomial model.</strong></p>
<p><strong>Let’s use 55 percent of the data for training and the rest for testing:</strong></p>
<pre class="ipython3"><code>x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)</code></pre>
<p><strong>We will perform a degree 5 polynomial transformation on the feature ‘horse power’.</strong></p>
<pre class="ipython3"><code>pr = PolynomialFeatures(degree=5)
x_train_pr = pr.fit_transform(x_train[[&#39;horsepower&#39;]])  
x_test_pr = pr.fit_transform(x_test[[&#39;horsepower&#39;]])
pr</code></pre>
<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PolynomialFeatures(degree=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" checked><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures(degree=5)</pre></div></div></div></div></div>
<p><strong>Now let’s create a linear regression model ``poly`` and train it.</strong></p>
<pre class="ipython3"><code>poly = LinearRegression()
poly.fit(x_train_pr, y_train)</code></pre>
<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-14" type="checkbox" checked><label for="sk-estimator-id-14" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
<p><strong>We can see the output of our model using the method ``predict.`` then assign the values to ``yhat``.</strong></p>
<pre class="ipython3"><code>yhat = poly.predict(x_test_pr)
yhat[0:5]</code></pre>
<div class="parsed-literal">
<dl>
<dt>array([ 6727.58267154, 7306.70989608, 12213.7082022 , 18895.03221601,</dt>
<dd><p>19996.83388031])</p>
</dd>
</dl>
</div>
<p><strong>Let’s take the first five predicted values and compare it to the actual targets.</strong></p>
<pre class="ipython3"><code>print(&quot;Predicted values:&quot;, yhat[0:4])
print(&quot;True values:&quot;, y_test[0:4].values)</code></pre>
<div class="parsed-literal">
<p>Predicted values: [ 6727.58267154 7306.70989608 12213.7082022 18895.03221601] True values: [ 6295. 10698. 13860. 13499.]</p>
</div>
<p><strong>We will use the function ``PollyPlot`` that we defined at the beginning of the section to display the training data, testing data, and the predicted function.</strong></p>
<pre class="ipython3"><code>PollyPlot(x_train[[&#39;horsepower&#39;]], x_test[[&#39;horsepower&#39;]], y_train, y_test, poly,pr)</code></pre>
<p><img src="output_306_0.png" alt="image" /></p>
<p>Figur 4 A polynomial regression model, red dots represent training data, green dots represent test data, and the blue line represents the model prediction.</p>
<p>We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points.</p>
<p>R^2 of the training data:</p>
<pre class="ipython3"><code>poly.score(x_train_pr, y_train)</code></pre>
<div class="parsed-literal">
<p>0.5568527853911671</p>
</div>
<p><strong>R^2 of the test data:</strong></p>
<pre class="ipython3"><code>poly.score(x_test_pr, y_test)</code></pre>
<div class="parsed-literal">
<p>-29.815558971873394</p>
</div>
<p><strong>We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87. The lower the R^2, the worse the model, a Negative R^2 is a sign of overfitting.</strong></p>
<p><strong>Let’s see how the R^2 changes on the test data for different order polynomials and plot the results:</strong></p>
<pre class="ipython3"><code>Rsqu_test = []

order = [1, 2, 3, 4]
for n in order:
    pr = PolynomialFeatures(degree=n)

    x_train_pr = pr.fit_transform(x_train[[&#39;horsepower&#39;]])

    x_test_pr = pr.fit_transform(x_test[[&#39;horsepower&#39;]])    

    lr.fit(x_train_pr, y_train)

    Rsqu_test.append(lr.score(x_test_pr, y_test))

plt.plot(order, Rsqu_test)
plt.xlabel(&#39;order&#39;)
plt.ylabel(&#39;R^2&#39;)
plt.title(&#39;R^2 Using Test Data&#39;)
plt.text(3, 0.75, &#39;Maximum R^2 &#39;)    </code></pre>
<div class="parsed-literal">
<p>Text(3, 0.75, 'Maximum R^2 ')</p>
</div>
<p><img src="output_312_1.png" alt="image" /></p>
<p><strong>We see the R^2 gradually increases until an order three polynomial is used. Then the R^2 dramatically decreases at four.</strong></p>
<p><strong>Let’s define a function f which takes two inputs, 1. order of polynomial and 2. % of test data, The functon f will be served to interact for creating interacting visuals.</strong></p>
<pre class="ipython3"><code>def f(order, test_data):
    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)
    pr = PolynomialFeatures(degree=order)
    x_train_pr = pr.fit_transform(x_train[[&#39;horsepower&#39;]])
    x_test_pr = pr.fit_transform(x_test[[&#39;horsepower&#39;]])
    poly = LinearRegression()
    poly.fit(x_train_pr,y_train)
    PollyPlot(x_train[[&#39;horsepower&#39;]], x_test[[&#39;horsepower&#39;]], y_train,y_test, poly, pr)</code></pre>
<p><strong>The following interface allows you to experiment with different polynomial orders and different amounts of data.</strong></p>
<pre class="ipython3"><code>interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))</code></pre>
<div class="parsed-literal">
<p>interactive(children=(IntSlider(value=3, description='order', max=6), FloatSlider(value=0.45, description='tes…</p>
</div>
<div class="parsed-literal">
<p>&lt;function __main__.f(order, test_data)&gt;</p>
</div>
<p><strong>Let’s perform polynomial transformation with more than one feature -‘horsepower’, ‘curb-weight’, ‘engine-size’, ‘highway-mpg’ and degree 2</strong></p>
<pre class="ipython3"><code>pr1 = PolynomialFeatures(degree = 2)

# Transform the training and testing samples for the features &#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39; and &#39;highway-mpg&#39;
x_train_pr1 = pr1.fit_transform(x_train[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]])
x_test_pr1 = pr1.fit_transform(x_test[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]])

print(&#39;How many dimensions does the new feature have?&#39;, x_train_pr1.shape)

# Create a linear regression model &quot;poly1&quot; and train the object using the method &quot;fit&quot; using the polynomial features
poly1=LinearRegression().fit(x_train_pr1,y_train)

# Use the method &quot;predict&quot; to predict an output on the polynomial features, 
# then use the function &quot;DistributionPlot&quot; to display the distribution of the predicted output vs the test data
yhat_test1 = poly1.predict(x_test_pr1)
Title=&#39;Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data&#39;

DistributionPlot(y_test, yhat_test1, &quot;Actual Values (Test)&quot;, &quot;Predicted Values (Test)&quot;, Title)</code></pre>
<div class="parsed-literal">
<p>How many dimensions does the new feature have? (110, 15)</p>
</div>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_319_2.png" alt="image" /></p>
<p>The predicted value is higher than actual value for cars where the price $10,000 range,</p>
<p>conversely the predicted price is lower than the price cost in the $30,000  to  $40,000 range.</p>
<p>As such the model is not as accurate in these ranges</p>
<h3 id="part-3-ridge-regression"><code>Part 3: Ridge regression</code></h3>
<p>In this section, we will review Ridge Regression we will see how the parameter <code>Alfa</code> changes the model. Just a note here our test data will be used as validation data.</p>
<p>Let’s perform a degree two polynomial transformation on our data.</p>
<pre class="ipython3"><code>pr=PolynomialFeatures(degree=2)
x_train_pr=pr.fit_transform(x_train[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;,&#39;normalized-losses&#39;,&#39;symboling&#39;]])
x_test_pr=pr.fit_transform(x_test[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;,&#39;normalized-losses&#39;,&#39;symboling&#39;]])</code></pre>
<p><strong>Let’s import Ridge from the module linear models.</strong></p>
<pre class="ipython3"><code>from sklearn.linear_model import Ridge</code></pre>
<p><strong>check the predicted and test values obtained by ridge model</strong></p>
<pre class="ipython3"><code># Let&#39;s create a Ridge regression object, setting the regularization parameter to 0.1
RigeModel=Ridge(alpha=0.1)

# Like regular regression, you can fit the model using the method fit.
RigeModel.fit(x_train_pr, y_train)

# Similarly, you can obtain a prediction:
yhat = RigeModel.predict(x_test_pr)

# Let&#39;s compare the first five predicted samples to our test set
print(&#39;predicted:&#39;, yhat[0:4])
print(&#39;test set :&#39;, y_test[0:4].values)</code></pre>
<div class="parsed-literal">
<p>predicted: [ 6569.10080596 9595.9695303 20834.19869602 19347.43557722] test set : [ 6295. 10698. 13860. 13499.]</p>
</div>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagessklearnlinear_model_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.02972e-16): result may not be accurate.</dt>
<dd><p>return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T</p>
</dd>
</dl>
</div>
<p><strong>We select the value of Alpha that minimizes the test error, for example, we can use a for loop.</strong></p>
<pre class="ipython3"><code>Rsqu_test = []
Rsqu_train = []
dummy1 = []
Alpha = 10 * np.array(range(0,1000))
for alpha in Alpha:
    RigeModel = Ridge(alpha=alpha) 
    RigeModel.fit(x_train_pr, y_train)
    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))
    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))</code></pre>
<p><strong>We can plot out the value of R^2 for different Alphas</strong></p>
<pre class="ipython3"><code>width = 12
height = 10
plt.figure(figsize=(width, height))

plt.plot(Alpha,Rsqu_test, label=&#39;validation data  &#39;)
plt.plot(Alpha,Rsqu_train, &#39;r&#39;, label=&#39;training Data &#39;)
plt.xlabel(&#39;alpha&#39;)
plt.ylabel(&#39;R^2&#39;)
plt.legend()</code></pre>
<div class="parsed-literal">
<p>&lt;matplotlib.legend.Legend at 0x1b6e242bdf0&gt;</p>
</div>
<p><img src="output_330_1.png" alt="image" /></p>
<p>Figure 6:The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha.</p>
<p>Here the model is built and tested on the same data. So the training and test data are the same.</p>
<p>The red line in figure 6 represents the R^2 of the training data. As Alpha increases the R^2 decreases. Therefore as Alpha increases the model performs worse on the training data.</p>
<p>The blue line represents the R^2 on the validation data. As the value for Alpha increases the R^2 increases and converges at a point</p>
<h1 id="part-4-grid-search"><code>Part 4: Grid Search</code></h1>
<p>The term Alfa is a hyperparameter, sklearn has the class GridSearchCV to make the process of finding the best hyperparameter simpler.</p>
<p>Let’s import GridSearchCV from the module model_selection.</p>
<pre class="ipython3"><code>from sklearn.model_selection import GridSearchCV</code></pre>
<p><strong>We create a dictionary of parameter values:</strong></p>
<pre class="ipython3"><code>parameters1= [{&#39;alpha&#39;: [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]
parameters1</code></pre>
<div class="parsed-literal">
<p>[{'alpha': [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]</p>
</div>
<p><strong>Let’s create ridge grid search object and train the model ove (‘horsepower’, ‘curb-weight’, ‘engine-size’, ‘highway-mpg’) with folds 4</strong></p>
<pre class="ipython3"><code># Create a ridge regions object:
RR=Ridge()

# Create a ridge grid search object
Grid1 = GridSearchCV(RR, parameters1,cv=4)

# Fit the model
Grid1.fit(x_data[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]], y_data)</code></pre>
<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=4, estimator=Ridge(),
             param_grid=[{&#x27;alpha&#x27;: [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000,
                                    100000]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-15" type="checkbox" ><label for="sk-estimator-id-15" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=4, estimator=Ridge(),
             param_grid=[{&#x27;alpha&#x27;: [0.001, 0.1, 1, 10, 100, 1000, 10000, 100000,
                                    100000]}])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-16" type="checkbox" ><label for="sk-estimator-id-16" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Ridge</label><div class="sk-toggleable__content"><pre>Ridge()</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-17" type="checkbox" ><label for="sk-estimator-id-17" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div>
<p><strong>The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:</strong></p>
<pre class="ipython3"><code>BestRR=Grid1.best_estimator_
BestRR</code></pre>
<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-10" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Ridge(alpha=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-18" type="checkbox" checked><label for="sk-estimator-id-18" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge(alpha=10000)</pre></div></div></div></div></div>
<p><strong>We now test our model on the test data</strong></p>
<pre class="ipython3"><code>BestRR.score(x_test[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]], y_test)</code></pre>
<div class="parsed-literal">
<p>0.841178138779323</p>
</div>
<p><strong>We now test our model on the training data</strong></p>
<pre class="ipython3"><code>BestRR.score(x_train[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]], y_train)</code></pre>
<div class="parsed-literal">
<p>0.7678682438637905</p>
</div>
<p><strong>Let’s visualize our model wihh alpha = 10000</strong></p>
<pre class="ipython3"><code>yhat = BestRR.predict(x_test[[&#39;horsepower&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;highway-mpg&#39;]])
Title=&#39;Distribution  Plot of Test Data vs Predicted Test Data with alpha 10000&#39;

DistributionPlot(y_test, yhat, &quot;Actual Values (Test)&quot;, &quot;Predicted Values (Test)&quot;, Title)</code></pre>
<div class="parsed-literal">
<dl>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
<dt>c:UserslenovoAppDataLocalProgramsPythonPython39libsite-packagesseaborndistributions.py:2619: FutureWarning: <span class="title-ref">distplot</span> is a deprecated function and will be removed in a future version. Please adapt your code to use either <span class="title-ref">displot</span> (a figure-level function with similar flexibility) or <span class="title-ref">kdeplot</span> (an axes-level function for kernel density plots).</dt>
<dd><p>warnings.warn(msg, FutureWarning)</p>
</dd>
</dl>
</div>
<p><img src="output_345_1.png" alt="image" /></p>
<h2 id="conclusion"><code>Conclusion</code></h2>
<p>Our model with the inputs - (‘horsepower’, ‘curb-weight’, ‘engine-size’, ‘highway-mpg’) can predict the Price of the second hand car with the efficiency of 84%</p>
